{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper Implementation\n",
    "\n",
    "This notebook comprises the code for our reimplementation of the\n",
    "[paper titled 'Entity Embeddings of Categorical Variables'](https://arxiv.org/pdf/1604.06737.pdf).\n",
    "The original code for the paper is available [here](https://github.com/entron/entity-embedding-rossmann)\n",
    "and is written using the Keras framework, whereas we used PyTorch. We referred to the\n",
    "original code to try and implement the paper as closely as possible.\n",
    "\n",
    "The experiment includes implementing neural network, random forest, gradient\n",
    "boosted trees, and KNN models, each with and without entity embeddings. We have implemented only\n",
    "the neural networks here as they are the core of the paper.\n",
    "\n",
    "Our results are close to theirs and this makes us confident that our implementation is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Unlike the paper, we use `pandas` to help with preprocessing because it is easier.\n",
    "The goal is to have a dataset with variables in TABLE 1 of the paper.\n",
    "The steps we take:\n",
    "1) Read in the dataset, only the columns that we will need.\n",
    "1) Drop rows where `Sales` (the dependent variable) is zero.\n",
    "1) Replace `Date` with `Day`, `Month`, and `Year`.\n",
    "1) Merge in the `State` column from a separate sheet.\n",
    "1) Encode all categorical columns into numeric types.\n",
    "    These are all the columns except `Sales`.\n",
    "1) Reorder the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 1.\n",
    "relevant_columns = [\"Store\", \"DayOfWeek\", \"Date\", \"Sales\", \"Promo\"]\n",
    "dataset = pd.read_csv(\"rossmann-store-sales/train.csv\", usecols=relevant_columns)\n",
    "\n",
    "# 2.\n",
    "dataset = dataset[dataset[\"Sales\"] != 0]\n",
    "\n",
    "# 3.\n",
    "dataset[[\"Year\", \"Month\", \"Day\"]] = dataset[\"Date\"].str.split(\"-\", expand=True)\n",
    "dataset.drop(columns=[\"Date\"], inplace=True)\n",
    "\n",
    "# 4.\n",
    "state_df = pd.read_csv(\"rossmann-store-sales/store_states.csv\")\n",
    "dataset = pd.merge(dataset, state_df, how=\"left\", on=\"Store\")\n",
    "del state_df # We delete large variables to save memory\n",
    "\n",
    "# 5.\n",
    "label_encoder = LabelEncoder()\n",
    "for col in dataset.columns.difference([\"Sales\"]):\n",
    "    dataset[col] = label_encoder.fit_transform(dataset[col])\n",
    "\n",
    "# 6.\n",
    "dataset = dataset[[\"Store\", \"DayOfWeek\", \"Day\", \"Month\", \"Year\", \"Promo\", \"State\", \"Sales\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Tensors\n",
    "\n",
    "We need 2 copies of the dataset here, which defer in their ordering:\n",
    "- Shuffled set\n",
    "    * Better for benchmarking models on statistical prediction accuracy\n",
    "- Temporal set\n",
    "    * Preseves time ordering, better for measuring generalizability of the models.\n",
    "        The original data was in reverse chronological order so we simply reverse it.\n",
    "\n",
    "Next, since its range is very large, we apply a log tranformation on the\n",
    "dependant variable to make it consistent with the output range of our models.\n",
    "We encapsulate the code to do it in a `OutputEncoder` class to make it easier to\n",
    "do this transformation in both directions.\n",
    "\n",
    "Finally we create a function to create the test/train splits. Important to not\n",
    "that for training, 200k rows are randomly sampled rather than the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Shuffled set\n",
    "shuffled_set = dataset.sample(frac=1)\n",
    "shuffled_set = torch.tensor(shuffled_set.values, dtype=torch.float)\n",
    "\n",
    "# Time sorted set\n",
    "temporal_set = dataset.iloc[::-1].copy()\n",
    "temporal_set = torch.tensor(temporal_set.values, dtype=torch.float)\n",
    "\n",
    "del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputEncoder():\n",
    "    def __init__(self, max_output):\n",
    "        self.max_output = max_output\n",
    "\n",
    "    def encode(self, output):\n",
    "        with torch.no_grad():\n",
    "            return torch.log(output) / torch.log(self.max_output)\n",
    "\n",
    "    def decode(self, output):\n",
    "        with torch.no_grad():\n",
    "            return torch.exp(output * torch.log(self.max_output))\n",
    "\n",
    "output_encoder = OutputEncoder(torch.max(temporal_set[:, -1]))\n",
    "\n",
    "temporal_set[:, -1] = output_encoder.encode(temporal_set[:, -1])\n",
    "shuffled_set[:, -1] = output_encoder.encode(shuffled_set[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_train_split(dataset):\n",
    "    split_threshold = int(0.9 * dataset.size(0))\n",
    "\n",
    "    X_train = dataset[:split_threshold, :-1].long()\n",
    "    X_test = dataset[split_threshold:, :-1].long()\n",
    "\n",
    "    y_train = dataset[:split_threshold, -1]\n",
    "    y_test = dataset[split_threshold:, -1]\n",
    "\n",
    "    train_indices = torch.randperm(X_train.size(0))[:200_000]\n",
    "    return X_train[train_indices], y_train[train_indices], X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Neural Networks\n",
    "\n",
    "The architecture and all hyperparameters are copied from the paper.  The input\n",
    "is converted to either one hot representation or passed through an embedding\n",
    "layer before being send to a typical feedforward neural network.\n",
    "\n",
    "About the implementation, we create a parameters dictionary that contains the\n",
    "number of unique values and embedding dimension for each variable. These numbers\n",
    "are from TABLE 1 of the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {parameter_name: (unique_values, embedding_dimension)}\n",
    "parameters = {\n",
    "    \"store\": (1115, 10),\n",
    "    \"day_of_week\": (7, 6),\n",
    "    \"day\": (31, 10),\n",
    "    \"month\": (12, 6),\n",
    "    \"year\": (3, 2),\n",
    "    \"promotion\": (2, 1),\n",
    "    \"state\": (12, 6)\n",
    "}\n",
    "    \n",
    "class EmbeddingNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EmbeddingNN, self).__init__()\n",
    "\n",
    "        emb_list = [torch.nn.Embedding(n, d) for n, d in parameters.values()]\n",
    "        self.emb_layers = torch.nn.ModuleList(emb_list)\n",
    "\n",
    "        input_size = sum([tuple[1] for tuple in parameters.values()])\n",
    "\n",
    "        self.feed_forward = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_size, 1000),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(1000, 500),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(500, 1),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        embeddings = torch.cat([emb(X[:, i]) for i, emb in enumerate(self.emb_layers)], dim=1)\n",
    "        \n",
    "        return self.feed_forward(embeddings)\n",
    "\n",
    "class OneHotNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OneHotNN, self).__init__()\n",
    "        input_size = sum([tuple[0] for tuple in parameters.values()])\n",
    "\n",
    "        self.feed_forward = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_size, 1000),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(1000, 500),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(500, 1),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        one_hot = torch.cat([torch.nn.functional.one_hot(X[:, i], num_emb).float()\n",
    "                             for i, (num_emb, _) in enumerate(parameters.values())], dim=1)\n",
    "\n",
    "        return self.feed_forward(one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing\n",
    "\n",
    "Learning rate, batch size, and number of epochs are copied from the paper. We\n",
    "use the `MAPE` metric for scoring. And for final evaluation, 5 models are\n",
    "trained then their predictions averaged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X, y):\n",
    "    loss_fn = torch.nn.L1Loss()\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    epochs = 10\n",
    "    batch_size = 128\n",
    "    total_samples = len(X)\n",
    "\n",
    "    model.train()\n",
    "    for _ in range(epochs):\n",
    "        for i in range(0, total_samples, batch_size):\n",
    "            inputs = X[i:i+batch_size]\n",
    "            targets = y[i:i+batch_size]\n",
    "            \n",
    "            optim.zero_grad()\n",
    "            outputs = model(inputs).squeeze()\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "def MAPE(y_pred, y_true):\n",
    "    return torch.mean(torch.abs((y_true - y_pred) / y_true))\n",
    "\n",
    "def evaluate(cls, dataset):\n",
    "    X_train, y_train, X_test, y_test = test_train_split(dataset)\n",
    "\n",
    "    models = [cls() for _ in range(5)]\n",
    "    for model in models:\n",
    "        train_model(model, X_train, y_train)\n",
    "\n",
    "    y_preds = []\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "        y_pred = model(X_test).squeeze()\n",
    "        y_pred = output_encoder.decode(y_pred)\n",
    "        y_preds.append(y_pred)\n",
    "\n",
    "    stacked_preds = torch.stack(y_preds)\n",
    "    y_pred = torch.mean(stacked_preds, dim=0)\n",
    "\n",
    "    y_true = output_encoder.decode(y_test)\n",
    "    return MAPE(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffled OneHotNN: 0.075\n",
      "Shuffled EmbeddingNN: 0.087\n",
      "Temporal OneHotNN: 0.103\n",
      "Temporal EmbeddingNN: 0.105\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shuffled OneHotNN: {evaluate(OneHotNN, shuffled_set):.3f}\")\n",
    "print(f\"Shuffled EmbeddingNN: {evaluate(EmbeddingNN, shuffled_set):.3f}\")\n",
    "print(f\"Temporal OneHotNN: {evaluate(OneHotNN, temporal_set):.3f}\")\n",
    "print(f\"Temporal EmbeddingNN: {evaluate(EmbeddingNN, temporal_set):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "We get the following results. Note that the numbers are `MAPE` score.\n",
    "\n",
    "|  | OneHotNN | EmbeddingNN |\n",
    "| --- | --- | --- |\n",
    "| Shuffled Data | 0.075 | 0.087 |\n",
    "| Temporal Data | 0.103 | 0.105 |\n",
    "\n",
    "Compare with the paper's results\n",
    "\n",
    "|  | OneHotNN | EmbeddingNN |\n",
    "| --- | --- | --- |\n",
    "| Shuffled Data | 0.070 | 0.070 |\n",
    "| Temporal Data | 0.101 | 0.093 |\n",
    "\n",
    "Our results are ok, but something to note is that we could not replicate the\n",
    "paper's results consistently.  We believe it to be a difference between PyTorch\n",
    "and Keras (or us not able to use PyTorch properly). To prove it, we\n",
    "implement the EmbeddingNN with Keras and see what results we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-04 17:41:23.010654: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-04 17:41:23.677536: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  15/1563 [..............................] - ETA: 45s - loss: 0.1610"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 72s 46ms/step - loss: 0.0143\n",
      "Epoch 2/10\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.0092\n",
      "Epoch 3/10\n",
      "1563/1563 [==============================] - 77s 49ms/step - loss: 0.0083\n",
      "Epoch 4/10\n",
      "1563/1563 [==============================] - 64s 41ms/step - loss: 0.0079\n",
      "Epoch 5/10\n",
      "1563/1563 [==============================] - 73s 46ms/step - loss: 0.0076\n",
      "Epoch 6/10\n",
      "1563/1563 [==============================] - 63s 40ms/step - loss: 0.0074\n",
      "Epoch 7/10\n",
      "1563/1563 [==============================] - 82s 52ms/step - loss: 0.0072\n",
      "Epoch 8/10\n",
      "1563/1563 [==============================] - 78s 50ms/step - loss: 0.0071\n",
      "Epoch 9/10\n",
      "1563/1563 [==============================] - 85s 55ms/step - loss: 0.0069\n",
      "Epoch 10/10\n",
      "1563/1563 [==============================] - 73s 47ms/step - loss: 0.0068\n",
      "2639/2639 [==============================] - 31s 12ms/step\n",
      "Temporal Keras_EmbeddingNN: 0.506\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Embedding, Concatenate, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "input_layer = Input(shape=(len(parameters),))\n",
    "\n",
    "embedding_layers = []\n",
    "for i, (n, d) in enumerate(parameters.values()):\n",
    "    embedding_layer = Embedding(n, d)(input_layer[:, i])\n",
    "    embedding_layers.append(embedding_layer)\n",
    "    \n",
    "concatenated = Concatenate()(embedding_layers)\n",
    "dense_layer_1 = Dense(1000, activation='relu')(concatenated)\n",
    "dense_layer_2 = Dense(500, activation='relu')(dense_layer_1)\n",
    "output_layer = Dense(1, activation='sigmoid')(dense_layer_2)\n",
    "\n",
    "keras_EmbeddingNN = Model(inputs=input_layer, outputs=output_layer)\n",
    "keras_EmbeddingNN.compile(optimizer='adam', loss='mean_absolute_error')\n",
    "\n",
    "X_train, y_train, X_test, y_test = map(lambda x: x.numpy(), test_train_split(temporal_set))\n",
    "keras_EmbeddingNN.fit(X_train, y_train, epochs=10, batch_size=128)\n",
    "\n",
    "y_pred = keras_EmbeddingNN.predict(X_test)\n",
    "y_pred = output_encoder.decode(torch.tensor(y_pred))\n",
    "y_true = output_encoder.decode(torch.tensor(y_test))\n",
    "\n",
    "print(f\"Temporal Keras_EmbeddingNN: {MAPE(y_pred, y_true):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first strange thing to note is the training time. We trained total 20 models\n",
    "for PyTorch and it took about 10 minutes, while the single Keras model took 13\n",
    "minutes. The score is also much better than anything we saw with PyTorch, even\n",
    "better than what they have stated in the paper. We do not understand these\n",
    "results currently, the models look identical but show vastly different\n",
    "behaviour."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
