{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper Implementation\n",
    "\n",
    "This notebook comprises the code for our reimplementation of the\n",
    "[paper titled 'Entity Embeddings of Categorical Variables'](https://arxiv.org/pdf/1604.06737.pdf).\n",
    "The original code for the paper is available [here](https://github.com/entron/entity-embedding-rossmann)\n",
    "and is written using the Keras framework, whereas we used PyTorch. We referred to the\n",
    "original code to try and implement the paper as closely as possible.\n",
    "\n",
    "The experiment includes implementing neural network, random forest, gradient\n",
    "boosted trees, and KNN models, each with and without entity embeddings. We have implemented only\n",
    "the neural networks here as they are the core of the paper.\n",
    "\n",
    "Our results are close to theirs and this makes us confident that our implementation is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "We utilize the `pandas` library for data preprocessing, aiming to structure our dataset according to the variables outlined in TABLE 1 of the paper.\n",
    "Here are the steps we followed for data preprocessing.\n",
    "## Steps\n",
    "\n",
    "1. **Remove Irrelevant Data:**\n",
    "   Select essential columns (`Store`, `DayOfWeek`, `Date`, `Sales`, `Promo`) to focus on pertinent information.\n",
    "\n",
    "2. **Handle Zero Sales:**\n",
    "   Exclude rows where sales (`Sales`) are zero to ensure a meaningful dataset.\n",
    "\n",
    "3. **Temporal Transformation:**\n",
    "   Transform the `Date` column into separate `Day`, `Month`, and `Year` columns for enhanced temporal analysis.\n",
    "\n",
    "4. **Include State Information:**\n",
    "   Integrate state information into the dataset, providing valuable geographical context. This step is also done in the paper.\n",
    "\n",
    "5. **Encode Categorical Variables:**\n",
    "   Convert categorical columns (excluding `Sales`) into numeric types for machine learning compatibility.\n",
    "\n",
    "6. **Reorder Columns:**\n",
    "   Adjust column order to enhance dataset clarity and alignment with desired structure.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 1. We read relevant columns from the CSV file\n",
    "relevant_columns = [\"Store\", \"DayOfWeek\", \"Date\", \"Sales\", \"Promo\"]\n",
    "dataset = pd.read_csv(\"rossmann-store-sales/train.csv\", usecols=relevant_columns)\n",
    "\n",
    "# 2. We filter out rows where Sales are zero (i.e. the store was closed)\n",
    "dataset = dataset[dataset[\"Sales\"] != 0]\n",
    "\n",
    "# 3. Extract Year, Month, and Day from the Date column. Then, drop the Date column. This gives us more categorical columns\n",
    "dataset[[\"Year\", \"Month\", \"Day\"]] = dataset[\"Date\"].str.split(\"-\", expand=True)\n",
    "dataset.drop(columns=[\"Date\"], inplace=True)\n",
    "\n",
    "# 4. Merge dataset with store states information. \n",
    "state_df = pd.read_csv(\"rossmann-store-sales/store_states.csv\")\n",
    "dataset = pd.merge(dataset, state_df, how=\"left\", on=\"Store\")\n",
    "del state_df  # We delete large variables to save memory\n",
    "\n",
    "# 5. Apply label encoding to categorical columns (excluding Sales). This just assigns a number to each category\n",
    "label_encoder = LabelEncoder()\n",
    "for col in dataset.columns.difference([\"Sales\"]):\n",
    "    dataset[col] = label_encoder.fit_transform(dataset[col])\n",
    "\n",
    "# 6. Lastly, we select final relevant columns for the dataset\n",
    "dataset = dataset[[\"Store\", \"DayOfWeek\", \"Day\", \"Month\", \"Year\", \"Promo\", \"State\", \"Sales\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Tensors\n",
    "\n",
    "In the process of data preparation for modeling, we follow a structured approach to generate effective datasets:\n",
    "\n",
    "**Shuffled Set and Temporal Set**\n",
    "\n",
    "We create two versions of our dataset to address distinct aspects of model evaluation:\n",
    "\n",
    "- **Shuffled Set:**\n",
    "  - Optimal for benchmarking models based on statistical prediction accuracy.\n",
    "\n",
    "- **Temporal Set:**\n",
    "  - Maintains the chronological order of the data, facilitating the assessment of model generalizability.\n",
    "  - Retains the original reverse chronological order of the data.\n",
    "\n",
    "**Log Transformation for Scaling**\n",
    "\n",
    "To address the wide range of the dependent variable, we apply a log transformation. This step ensures consistency with the output range of our models. The transformation code is conveniently encapsulated within the `OutputEncoder` class, allowing for easy execution of both forward and reverse transformations.\n",
    "\n",
    "**Train/Test Splits**\n",
    "\n",
    "Creating reliable training and testing sets is crucial for accurate model evaluation. We implement a function to achieve this, with an emphasis on:\n",
    "- Randomly sampling 200k rows during training for efficiency without compromising the model's robustness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Shuffled set\n",
    "# Randomly shuffle the dataset to create the shuffled set.\n",
    "shuffled_set = dataset.sample(frac=1)\n",
    "shuffled_set = torch.tensor(shuffled_set.values, dtype=torch.float)\n",
    "\n",
    "# Time sorted set\n",
    "# Reverse the chronological order to create the temporal set as the dataset is in reverse chronological order\n",
    "temporal_set = dataset.iloc[::-1].copy()\n",
    "temporal_set = torch.tensor(temporal_set.values, dtype=torch.float)\n",
    "\n",
    "# Clean up memory by deleting the original dataset.\n",
    "del dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputEncoder():\n",
    "    def __init__(self, max_output):\n",
    "        # Initialize the OutputEncoder with the maximum output value.\n",
    "        self.max_output = max_output\n",
    "\n",
    "    def encode(self, output):\n",
    "        # Logarithmically encode the output to handle a large output range.\n",
    "        with torch.no_grad():\n",
    "            return torch.log(output) / torch.log(self.max_output)\n",
    "\n",
    "    def decode(self, output):\n",
    "        # Decode the output by taking the exponential.\n",
    "        with torch.no_grad():\n",
    "            return torch.exp(output * torch.log(self.max_output))\n",
    "\n",
    "# Create an instance of OutputEncoder with the maximum value from temporal_set.\n",
    "output_encoder = OutputEncoder(torch.max(temporal_set[:, -1]))\n",
    "\n",
    "# Apply encoding to the last column (Sales) in both temporal_set and shuffled_set.\n",
    "temporal_set[:, -1] = output_encoder.encode(temporal_set[:, -1])\n",
    "shuffled_set[:, -1] = output_encoder.encode(shuffled_set[:, -1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_train_split(dataset):\n",
    "    # Define the split threshold for 90% training and 10% testing.\n",
    "    split_threshold = int(0.9 * dataset.size(0))\n",
    "\n",
    "    # Extract features (X) and target variable (y) for training and testing sets.\n",
    "    X_train = dataset[:split_threshold, :-1].long() \n",
    "    # We also convert to long dtype because in PyTorch, embedding layers typically expect input of type torch.long (or torch.int64). This is because embedding layers are designed to work with discrete indices, such as those used to represent categories or words.\n",
    "    X_test = dataset[split_threshold:, :-1].long()\n",
    "\n",
    "    y_train = dataset[:split_threshold, -1]\n",
    "    y_test = dataset[split_threshold:, -1]\n",
    "\n",
    "    # Randomly sampling 200,000 rows for training as done in paper\n",
    "    train_indices = torch.randperm(X_train.size(0))[:200_000]\n",
    "\n",
    "    # Return the sampled training data and full testing data.\n",
    "    return X_train[train_indices], y_train[train_indices], X_test, y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Architectures\n",
    "\n",
    "We implemented two distinct neural network models: `EmbeddingNN` and `OneHotNN`. Each model's architecture closely aligns with the specifications outlined in TABLE 1 of the paper, incorporating specific strategies for handling categorical variables.\n",
    "\n",
    "**EmbeddingNN**\n",
    "\n",
    "The `EmbeddingNN` model utilizes embedding layers for categorical variables, with embedding dimensions determined by the unique values of each variable. This design choice is reflected in the `parameters` dictionary, which encapsulates the necessary details. The overall structure comprises a standard feedforward neural network, strictly following the outlined hyperparameters.\n",
    "\n",
    "**OneHotNN**\n",
    "\n",
    "In contrast, the `OneHotNN` model processes input data using one-hot encoding. Similar to `EmbeddingNN`, this model follows the paper's architectural design and hyperparameter choices.\n",
    "\n",
    "*Model Architecture*\n",
    "\n",
    "Both models employ the learning rate, batch size, and number of epochs as specified in the original paper. The input data undergoes preprocessing, with categorical variables either converted to a one-hot representation or passed through embedding layers, depending on the model.\n",
    "\n",
    "*Parameters Dictionary*\n",
    "\n",
    "The `parameters` dictionary plays a crucial role in our implementation as it captures the unique values and embedding dimensions for each variable. We refer to TABLE 1 in the paper for these values, ensuring a precise replication of the specified neural network architectures.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters dictionary, where each key represents a feature and the associated tuple\n",
    "# contains the number of unique values and the embedding dimension for that feature.\n",
    "# {parameter_name: (unique_values, embedding_dimension)}\n",
    "parameters = {\n",
    "    \"store\": (1115, 10),\n",
    "    \"day_of_week\": (7, 6),\n",
    "    \"day\": (31, 10),\n",
    "    \"month\": (12, 6),\n",
    "    \"year\": (3, 2),\n",
    "    \"promotion\": (2, 1),\n",
    "    \"state\": (12, 6)\n",
    "}\n",
    "\n",
    "\n",
    "class EmbeddingNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EmbeddingNN, self).__init__()\n",
    "\n",
    "        # Create a list of embedding layers based on the parameters\n",
    "        emb_list = [torch.nn.Embedding(n, d) for n, d in parameters.values()]\n",
    "        self.emb_layers = torch.nn.ModuleList(emb_list)\n",
    "\n",
    "        # Calculate the total input size for the feedforward network. This is equal to the sum of embedding dimensions\n",
    "        input_size = sum([tuple[1] for tuple in parameters.values()])\n",
    "\n",
    "        # Define the feedforward network with ReLU activations and a sigmoid output. This is done using the Sequential module\n",
    "        self.feed_forward = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_size, 1000),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(1000, 500),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(500, 1),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Concatenate the embeddings obtained from each categorical feature using list comprehension\n",
    "        # Iterate over the embedding layers and apply them to corresponding columns in the input X\n",
    "        # The resulting embeddings are concatenated along the specified dimension (dim=1) to form a single tensor\n",
    "        embeddings = torch.cat([emb(X[:, i])\n",
    "                               for i, emb in enumerate(self.emb_layers)], dim=1)\n",
    "\n",
    "        return self.feed_forward(embeddings) # Pass the concatenated embeddings to the feedforward network\n",
    "\n",
    "\n",
    "class OneHotNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OneHotNN, self).__init__()\n",
    "\n",
    "        # Calculate the total input size for the feedforward network using the number of unique values\n",
    "        input_size = sum([tuple[0] for tuple in parameters.values()])\n",
    "\n",
    "        # Define the feedforward network with ReLU activations and a sigmoid output as in paper\n",
    "        self.feed_forward = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_size, 1000),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(1000, 500),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(500, 1),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Convert input features into one-hot encoding and concatenate them\n",
    "        one_hot = torch.cat([torch.nn.functional.one_hot(X[:, i], num_emb).float()\n",
    "                             for i, (num_emb, _) in enumerate(parameters.values())], dim=1)\n",
    "\n",
    "        return self.feed_forward(one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing\n",
    "\n",
    "To maintain consistency with the referenced paper, we adopt the same values for\n",
    "learning rate, batch size, and the number of epochs. The Mean Absolute\n",
    "Percentage Error (MAPE) metric is utilized for model evaluation since it is more robust to outliers. We train five models for each class, and their predictions are averaged for a comprehensive evaluation. This ensemble approach provides a\n",
    "more robust and generalizable model, mitigating the impact of individual model\n",
    "idiosyncrasies and enhancing overall predictive performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X, y):\n",
    "    # Define loss function and optimizer\n",
    "    loss_fn = torch.nn.L1Loss()\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    epochs = 10\n",
    "    batch_size = 128\n",
    "    total_samples = len(X)\n",
    "\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    for _ in range(epochs):\n",
    "        for i in range(0, total_samples, batch_size):\n",
    "            # Get a batch of data\n",
    "            inputs = X[i:i+batch_size]\n",
    "            targets = y[i:i+batch_size]\n",
    "            # Zero the gradients, forward pass, backward pass, and optimization step\n",
    "            optim.zero_grad()\n",
    "            outputs = model(inputs).squeeze() # Squeeze to remove extra dimension\n",
    "            loss = loss_fn(outputs, targets) # Calculate loss\n",
    "            loss.backward() # Calculate gradients\n",
    "            optim.step() # Update the parameters\n",
    "\n",
    "def MAPE(y_pred, y_true):\n",
    "    return torch.mean(torch.abs((y_true - y_pred) / y_true)) \n",
    "\n",
    "def evaluate(cls, dataset):\n",
    "    # Split the dataset into train and test sets\n",
    "    X_train, y_train, X_test, y_test = test_train_split(dataset)\n",
    "\n",
    "    # Create and train 5 models\n",
    "    models = [cls() for _ in range(5)]\n",
    "    for model in models:\n",
    "        train_model(model, X_train, y_train)\n",
    "\n",
    "    # Make predictions and calculate ensemble prediction\n",
    "    y_preds = []\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "        y_pred = model(X_test).squeeze()\n",
    "        y_pred = output_encoder.decode(y_pred)\n",
    "        y_preds.append(y_pred)\n",
    "\n",
    "    stacked_preds = torch.stack(y_preds) # Stack along the first dimension. This is done to average the predictions\n",
    "    y_pred = torch.mean(stacked_preds, dim=0)\n",
    "\n",
    "    # Decode the true values and calculate MAPE\n",
    "    y_true = output_encoder.decode(y_test)\n",
    "    return MAPE(y_pred, y_true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffled OneHotNN: 0.075\n",
      "Shuffled EmbeddingNN: 0.087\n",
      "Temporal OneHotNN: 0.103\n",
      "Temporal EmbeddingNN: 0.105\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the Mean Absolute Percentage Error (MAPE) values for both OneHotNN and EmbeddingNN models on both datasets (shuffled and temporal)\n",
    "print(f\"Shuffled OneHotNN: {evaluate(OneHotNN, shuffled_set):.3f}\")\n",
    "print(f\"Shuffled EmbeddingNN: {evaluate(EmbeddingNN, shuffled_set):.3f}\")\n",
    "print(f\"Temporal OneHotNN: {evaluate(OneHotNN, temporal_set):.3f}\")\n",
    "print(f\"Temporal EmbeddingNN: {evaluate(EmbeddingNN, temporal_set):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "We obtained the following results, with the numbers representing the `MAPE` scores.\n",
    "\n",
    "|  | OneHotNN | EmbeddingNN |\n",
    "| --- | --- | --- |\n",
    "| Shuffled Data | 0.075 | 0.087 |\n",
    "| Temporal Data | 0.103 | 0.105 |\n",
    "\n",
    "### Comparison with Paper's Results\n",
    "\n",
    "|  | OneHotNN | EmbeddingNN |\n",
    "| --- | --- | --- |\n",
    "| Shuffled Data | 0.070 | 0.070 |\n",
    "| Temporal Data | 0.101 | 0.093 |\n",
    "\n",
    "While our obtained results demonstrate satisfactory performance, it's important to highlight a notable observation. We faced challenges in replicating the exact `MAPE` scores reported in the paper. We attribute this discrepancy to potential differences between PyTorch and Keras implementations or potential challenges in our PyTorch setup.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
