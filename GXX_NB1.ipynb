{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper Implementation\n",
    "\n",
    "This notebook comprises the code for our reimplementation of the\n",
    "[paper titled 'Entity Embeddings of Categorical Variables'](https://arxiv.org/pdf/1604.06737.pdf).\n",
    "The original code for the paper is available [here](https://github.com/entron/entity-embedding-rossmann)\n",
    "and is written using the Keras framework, whereas we used PyTorch. We referred to the\n",
    "original code to try and implement the paper as closely as possible.\n",
    "\n",
    "The experiment includes implementing neural network, random forest, gradient\n",
    "boosted trees, and KNN models, each with and without entity embeddings. We have implemented only\n",
    "the neural networks here as they are the core of the paper.\n",
    "\n",
    "Our results are close to theirs and this makes us confident that our implementation is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Unlike the paper, we use `pandas` to help with preprocessing because it is easier.\n",
    "The goal is to have a dataset with variables in TABLE 1 of the paper.\n",
    "The steps we take:\n",
    "1) Read in the dataset, only the columns that we will need.\n",
    "1) Drop rows where `Sales` (the dependent variable) is zero.\n",
    "1) Replace `Date` with `Day`, `Month`, and `Year`.\n",
    "1) Merge in the `State` column from a separate sheet.\n",
    "1) Encode all categorical columns into numeric types.\n",
    "    These are all the columns except `Sales`.\n",
    "1) Reorder the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 1.\n",
    "relevant_columns = [\"Store\", \"DayOfWeek\", \"Date\", \"Sales\", \"Promo\"]\n",
    "dataset = pd.read_csv(\"rossmann-store-sales/train.csv\", usecols=relevant_columns)\n",
    "\n",
    "# 2.\n",
    "dataset = dataset[dataset[\"Sales\"] != 0]\n",
    "\n",
    "# 3.\n",
    "dataset[[\"Year\", \"Month\", \"Day\"]] = dataset[\"Date\"].str.split(\"-\", expand=True)\n",
    "dataset.drop(columns=[\"Date\"], inplace=True)\n",
    "\n",
    "# 4.\n",
    "state_df = pd.read_csv(\"rossmann-store-sales/store_states.csv\")\n",
    "dataset = pd.merge(dataset, state_df, how=\"left\", on=\"Store\")\n",
    "del state_df # We delete large variables to save memory\n",
    "\n",
    "# 5.\n",
    "label_encoder = LabelEncoder()\n",
    "for col in dataset.columns.difference([\"Sales\"]):\n",
    "    dataset[col] = label_encoder.fit_transform(dataset[col])\n",
    "\n",
    "# 6.\n",
    "dataset = dataset[[\"Store\", \"DayOfWeek\", \"Day\", \"Month\", \"Year\", \"Promo\", \"State\", \"Sales\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Tensors\n",
    "\n",
    "We need 2 copies of the dataset here, which defer in their ordering:\n",
    "- Shuffled set\n",
    "    * Better for benchmarking models on statistical prediction accuracy\n",
    "- Temporal set\n",
    "    * Preseves time ordering, better for measuring generalizability of the models.\n",
    "        The original data was in reverse chronological order so we simply reverse it.\n",
    "\n",
    "Next, since its range is very large, we apply a log tranformation on the\n",
    "dependant variable to make it consistent with the output range of our models.\n",
    "We encapsulate the code to do it in a `OutputEncoder` class to make it easier to\n",
    "do this transformation in both directions.\n",
    "\n",
    "Finally we create a function to create the test/train splits. Important to not\n",
    "that for training, 200k rows are randomly sampled rather than the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Shuffled set\n",
    "shuffled_set = dataset.sample(frac=1)\n",
    "shuffled_set = torch.tensor(shuffled_set.values, dtype=torch.float)\n",
    "\n",
    "# Time sorted set\n",
    "temporal_set = dataset.iloc[::-1].copy()\n",
    "temporal_set = torch.tensor(temporal_set.values, dtype=torch.float)\n",
    "\n",
    "del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputEncoder():\n",
    "    def __init__(self, max_output):\n",
    "        self.max_output = max_output\n",
    "\n",
    "    def encode(self, output):\n",
    "        with torch.no_grad():\n",
    "            return torch.log(output) / torch.log(self.max_output)\n",
    "\n",
    "    def decode(self, output):\n",
    "        with torch.no_grad():\n",
    "            return torch.exp(output * torch.log(self.max_output))\n",
    "\n",
    "output_encoder = OutputEncoder(torch.max(temporal_set[:, -1]))\n",
    "\n",
    "temporal_set[:, -1] = output_encoder.encode(temporal_set[:, -1])\n",
    "shuffled_set[:, -1] = output_encoder.encode(shuffled_set[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_train_split(dataset):\n",
    "    split_threshold = int(0.9 * dataset.size(0))\n",
    "\n",
    "    X_train = dataset[:split_threshold, :-1].long()\n",
    "    X_test = dataset[split_threshold:, :-1].long()\n",
    "\n",
    "    y_train = dataset[:split_threshold, -1]\n",
    "    y_test = dataset[split_threshold:, -1]\n",
    "\n",
    "    train_indices = torch.randperm(X_train.size(0))[:200_000]\n",
    "    return X_train[train_indices], y_train[train_indices], X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Neural Networks\n",
    "\n",
    "The architecture and all hyperparameters are copied from the paper.  The input\n",
    "is converted to either one hot representation or passed through an embedding\n",
    "layer before being send to a typical feedforward neural network.\n",
    "\n",
    "About the implementation, we create a parameters dictionary that contains the\n",
    "number of unique values and embedding dimension for each variable. These numbers\n",
    "are from TABLE 1 of the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {parameter_name: (unique_values, embedding_dimension)}\n",
    "parameters = {\n",
    "    \"store\": (1115, 10),\n",
    "    \"day_of_week\": (7, 6),\n",
    "    \"day\": (31, 10),\n",
    "    \"month\": (12, 6),\n",
    "    \"year\": (3, 2),\n",
    "    \"promotion\": (2, 1),\n",
    "    \"state\": (12, 6)\n",
    "}\n",
    "    \n",
    "class EmbeddingNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EmbeddingNN, self).__init__()\n",
    "\n",
    "        emb_list = [torch.nn.Embedding(n, d) for n, d in parameters.values()]\n",
    "        self.emb_layers = torch.nn.ModuleList(emb_list)\n",
    "\n",
    "        input_size = sum([tuple[1] for tuple in parameters.values()])\n",
    "\n",
    "        self.feed_forward = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_size, 1000),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(1000, 500),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(500, 1),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        embeddings = torch.cat([emb(X[:, i]) for i, emb in enumerate(self.emb_layers)], dim=1)\n",
    "        \n",
    "        return self.feed_forward(embeddings)\n",
    "\n",
    "class OneHotNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OneHotNN, self).__init__()\n",
    "        input_size = sum([tuple[0] for tuple in parameters.values()])\n",
    "\n",
    "        self.feed_forward = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_size, 1000),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(1000, 500),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(500, 1),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        one_hot = torch.cat([torch.nn.functional.one_hot(X[:, i], num_emb).float()\n",
    "                             for i, (num_emb, _) in enumerate(parameters.values())], dim=1)\n",
    "\n",
    "        return self.feed_forward(one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing\n",
    "\n",
    "Learning rate, batch size, and number of epochs are copied from the paper. We\n",
    "use the `MAPE` metric for scoring. And for final evaluation, 5 models are\n",
    "trained then their predictions averaged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X, y):\n",
    "    loss_fn = torch.nn.L1Loss()\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    epochs = 10\n",
    "    batch_size = 128\n",
    "    total_samples = len(X)\n",
    "\n",
    "    model.train()\n",
    "    for _ in range(epochs):\n",
    "        for i in range(0, total_samples, batch_size):\n",
    "            inputs = X[i:i+batch_size]\n",
    "            targets = y[i:i+batch_size]\n",
    "            \n",
    "            optim.zero_grad()\n",
    "            outputs = model(inputs).squeeze()\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "def MAPE(models, X, y_true):\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "        \n",
    "    y_preds = [model(X).squeeze() for model in models]\n",
    "    y_preds = [output_encoder.decode(y_pred) for y_pred in y_preds]\n",
    "\n",
    "    stacked_preds = torch.stack(y_preds)\n",
    "    y_pred = torch.mean(stacked_preds, dim=0)\n",
    "\n",
    "    y_true = output_encoder.decode(y_true)\n",
    "    return torch.mean(torch.abs((y_true - y_pred) / y_true))\n",
    "\n",
    "def evaluate(cls, dataset):\n",
    "    X_train, y_train, X_test, y_test = test_train_split(dataset)\n",
    "\n",
    "    models = [cls() for _ in range(5)]\n",
    "    for model in models:\n",
    "        train_model(model, X_train, y_train)\n",
    "\n",
    "    return MAPE(models, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffled OneHotNN: 0.079\n",
      "Shuffled EmbeddingNN: 0.084\n",
      "Temporal OneHotNN: 0.110\n",
      "Temporal EmbeddingNN: 0.109\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shuffled OneHotNN: {evaluate(OneHotNN, shuffled_set):.3f}\")\n",
    "print(f\"Shuffled EmbeddingNN: {evaluate(EmbeddingNN, shuffled_set):.3f}\")\n",
    "print(f\"Temporal OneHotNN: {evaluate(OneHotNN, temporal_set):.3f}\")\n",
    "print(f\"Temporal EmbeddingNN: {evaluate(EmbeddingNN, temporal_set):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "We get the following results. Note that the numbers are `MAPE` score.\n",
    "\n",
    "|  | OneHotNN | EmbeddingNN |\n",
    "| --- | --- | --- |\n",
    "| Shuffled Data | 0.079 | 0.084 |\n",
    "| Temporal Data | 0.110 | 0.109 |\n",
    "\n",
    "Compare with the paper's results\n",
    "\n",
    "|  | OneHotNN | EmbeddingNN |\n",
    "| --- | --- | --- |\n",
    "| Shuffled Data | 0.070 | 0.070 |\n",
    "| Temporal Data | 0.101 | 0.093 |\n",
    "\n",
    "Our results are look good, slightly worse than their's. Maybe it's a difference\n",
    "between Keras and PyTorch where some defaults are different. There can also be a\n",
    "mistake in our implementation, but we are satisfied."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
