{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "We use `pandas` to help with preprocessing.\n",
    "The goal is to have the variables in TABLE 1 of the paper and no more.\n",
    "The steps we take:\n",
    "1) Read in the dataset, only the columns that we will need.\n",
    "1) Drop rows where `Sales` (our output variable) is zero.\n",
    "1) Replace `Date` with `Day`, `Month`, and `Year`.\n",
    "1) Merge in the `State` column from a separate sheet.\n",
    "1) Encode all categorical columns into numeric types.\n",
    "    These are all the columns except `Sales`.\n",
    "1) Reorder the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 1.\n",
    "relevant_columns = [\"Store\", \"DayOfWeek\", \"Date\", \"Sales\", \"Promo\"]\n",
    "dataset = pd.read_csv(\"rossmann-store-sales/train.csv\", usecols=relevant_columns)\n",
    "\n",
    "# 2.\n",
    "dataset = dataset[dataset[\"Sales\"] != 0]\n",
    "\n",
    "# 3.\n",
    "dataset[[\"Year\", \"Month\", \"Day\"]] = dataset[\"Date\"].str.split(\"-\", expand=True)\n",
    "dataset.drop(columns=[\"Date\"], inplace=True)\n",
    "\n",
    "# 4.\n",
    "state_df = pd.read_csv(\"rossmann-store-sales/store_states.csv\")\n",
    "dataset = pd.merge(dataset, state_df, how=\"left\", on=\"Store\")\n",
    "del state_df # We delete large variables to save memory\n",
    "\n",
    "# 5.\n",
    "label_encoder = LabelEncoder()\n",
    "for col in dataset.columns.difference([\"Sales\"]):\n",
    "    dataset[col] = label_encoder.fit_transform(dataset[col])\n",
    "\n",
    "# 6.\n",
    "dataset = dataset[[\"Store\", \"DayOfWeek\", \"Day\", \"Month\", \"Year\", \"Promo\", \"State\", \"Sales\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Tensors\n",
    "\n",
    "Mostly straighforward. We highlight the notable bits.\n",
    "\n",
    "Firsly, the paper describes a transformation to make to the output variable so it is within the range of the sigmoid function. This normalization is done because the Sales column spans 4 orders of magnitudes, and hence doing so allows us to scale to the same range as\n",
    "the neural network output\n",
    "We create a `OutputEncoder` class to encapsulate this so it is easy to do this transformation in both directions.\n",
    "\n",
    "Next we do the test/train split. The paper describes two ways to do it:\n",
    "1) Preseving original temporal ordering \n",
    "    * Since, this way, the test data is of a future time whose probability distrubition has not yet been sampled by the model, it is a better predictor of the model's generalizabiltiy\n",
    "2) Shuffling the data \n",
    "    * This is beneficial for benchmarking model's performance based on its statistical prediction accuracy\n",
    "\n",
    "We implement both and state both results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "X = torch.tensor(dataset.drop(columns=[\"Sales\"]).values)\n",
    "y = torch.tensor(dataset[\"Sales\"].values, dtype=torch.float)\n",
    "del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputEncoder():\n",
    "    def __init__(self, max_output):\n",
    "        self.max_output = max_output\n",
    "\n",
    "    def encode(self, output):\n",
    "        with torch.no_grad():\n",
    "            return torch.log(output) / torch.log(self.max_output)\n",
    "\n",
    "    def decode(self, output):\n",
    "        with torch.no_grad():\n",
    "            return torch.exp(output * torch.log(self.max_output))\n",
    "\n",
    "output_encoder = OutputEncoder(torch.max(y))\n",
    "y = output_encoder.encode(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal split (Already in correct order)\n",
    "X_temporal = X.clone()\n",
    "y_temporal = y.clone()\n",
    "\n",
    "# Shuffled split\n",
    "shuffled_indices = torch.randperm(X.size(0))\n",
    "\n",
    "X_shuffled = X[shuffled_indices].clone()\n",
    "y_shuffled = y[shuffled_indices].clone()\n",
    "\n",
    "del X\n",
    "del y\n",
    "\n",
    "# Common split function\n",
    "def test_train_split(X, y):\n",
    "    split_threshold = int(0.9 * X.size(0))\n",
    "    \n",
    "    X_train = X[:split_threshold]\n",
    "    X_test = X[split_threshold:]\n",
    "\n",
    "    y_train = y[:split_threshold]\n",
    "    y_test = y[split_threshold:]\n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Neural Networks\n",
    "\n",
    "The architecture is defined well in the paper and we will follow it. The paper defines 2 ways to create the networks, with embeddings and with one-hot vectors. We again implement both ways.\n",
    "\n",
    "* One-hot encoding NN\n",
    "    * The NN with One-hot encodings creates one hot-encoded vectors for the inputs and feeds this into the model. It does not learn any intrinsic propertie/meanings for the features and only learns the output feature (Sales) distribuition based on the input features.\n",
    "* Entity Embeddings NN\n",
    "    * The NN with entity embeddings learns the embedding representation of each categorical feature. This means the model also learns the intrinsic properties/meanings of each feature along with the sales distribuition. This is beneficial when the model sees new data and is able to generalize to it better. It also means that the NN through entitiy emeddings restricts itself in a much smaller but meaningful parameter space. This reduces the chance that the network converges to local minimums far from the global minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EmbeddingNN, self).__init__()\n",
    "        # From TABLE 1. Each tuple is (unique_values, embedding_dimension)\n",
    "        emb_dims = [(1115, 10), (7, 6), (31, 10), (12, 6), (3, 2), (2, 1), (12, 6)]\n",
    "\n",
    "        self.embs = [torch.nn.Embedding(*args) for args in emb_dims]\n",
    "        self.fc1 = torch.nn.Linear(sum(dim for _, dim in emb_dims), 1000)\n",
    "        self.relu1 = torch.nn.ReLU()\n",
    "        self.fc2 = torch.nn.Linear(1000, 500)\n",
    "        self.relu2 = torch.nn.ReLU()\n",
    "        self.output = torch.nn.Linear(500, 1)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, X):\n",
    "        out = [emb(X[:, i].long()) for i, emb in enumerate(self.embs)] \n",
    "        out = torch.cat(out, dim=1)\n",
    "        \n",
    "        out = self.relu1(self.fc1(out)) \n",
    "        out = self.relu2(self.fc2(out))\n",
    "        out = self.sigmoid(self.output(out))\n",
    "        return out\n",
    "\n",
    "class OneHotNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OneHotNN, self).__init__()\n",
    "        # Required to create the one-hot vectors\n",
    "        self.one_hot_classes = [1115, 7, 31, 12, 3, 2, 12]\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(sum(self.one_hot_classes), 1000)\n",
    "        self.relu1 = torch.nn.ReLU()\n",
    "        self.fc2 = torch.nn.Linear(1000, 500)\n",
    "        self.relu2 = torch.nn.ReLU()\n",
    "        self.output = torch.nn.Linear(500, 1)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, X):\n",
    "        out = [torch.nn.functional.one_hot(X[:, i], num_class).float()\n",
    "                for i, num_class in enumerate(self.one_hot_classes)]\n",
    "        out = torch.cat(out, dim=1)\n",
    "\n",
    "        out = self.relu1(self.fc1(out))\n",
    "        out = self.relu2(self.fc2(out))\n",
    "        out = self.sigmoid(self.output(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Testing\n",
    "\n",
    "We create some functions to reduce repetitive code when creating multiple models and training on different data.\n",
    "Important things to note are that for predictions, 5 models are created and trained, then their predictions averaged, as mentioned in the paper.\n",
    "The `MAPE` (mean absolute percent error) metric is used for scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X, y):\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    epochs = 10\n",
    "    batch_size = 128\n",
    "    total_samples = len(X)\n",
    "\n",
    "    model.train()\n",
    "    for _ in range(epochs):\n",
    "        for i in range(0, total_samples, batch_size):\n",
    "            inputs = X[i:i+batch_size]\n",
    "            targets = y[i:i+batch_size]\n",
    "            \n",
    "            optim.zero_grad()\n",
    "            outputs = model(inputs).squeeze()\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "def MAPE(models, X, y_true):\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "        \n",
    "    y_preds = [model(X).squeeze() for model in models]\n",
    "    stacked_preds = torch.stack(y_preds)\n",
    "    y_pred = torch.mean(stacked_preds, dim=0)\n",
    "\n",
    "    y_pred = output_encoder.decode(y_pred)\n",
    "    y_true = output_encoder.decode(y_true)\n",
    "\n",
    "    return torch.mean(torch.abs((y_true - y_pred) / y_true))\n",
    "\n",
    "def evaluate(cls, X, y):\n",
    "    X_train, y_train, X_test, y_test = test_train_split(X, y)\n",
    "\n",
    "    models = [cls() for _ in range(5)]\n",
    "    for model in models:\n",
    "        train_model(model, X_train, y_train)\n",
    "\n",
    "    return MAPE(models, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffled OneHotNN: 0.064\n",
      "Shuffled EmbeddingNN: 0.073\n",
      "Temporal OneHotNN: 0.122\n",
      "Temporal EmbeddingNN: 0.131\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shuffled OneHotNN: {evaluate(OneHotNN, X_shuffled, y_shuffled):.3f}\")\n",
    "print(f\"Shuffled EmbeddingNN: {evaluate(EmbeddingNN, X_shuffled, y_shuffled):.3f}\")\n",
    "print(f\"Temporal OneHotNN: {evaluate(OneHotNN, X_temporal, y_temporal):.3f}\")\n",
    "print(f\"Temporal EmbeddingNN: {evaluate(EmbeddingNN, X_temporal, y_temporal):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "We get the following results. Note that the numbers are `MAPE` score.\n",
    "\n",
    "|  | OneHotNN | EmbeddingNN |\n",
    "| --- | --- | --- |\n",
    "| Shuffled Data | 0.064 | 0.073 |\n",
    "| Temporal Data | 0.122 | 0.131 |\n",
    "\n",
    "Compare with the paper's results\n",
    "\n",
    "|  | OneHotNN | EmbeddingNN |\n",
    "| --- | --- | --- |\n",
    "| Shuffled Data | 0.070 | 0.070 |\n",
    "| Temporal Data | 0.101 | 0.093 |\n",
    "\n",
    "Our results are slightly worse than theirs. There might be a mistake in our implementation, but we could not find. Or it could just be random chance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
