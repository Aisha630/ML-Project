{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "We use `pandas` to help with preprocessing.\n",
    "The goal is to have the variables in TABLE 1 of the paper and no more.\n",
    "The steps we take:\n",
    "1) Read in the dataset, only the columns that we will need.\n",
    "1) Drop rows where `Sales` is either zero or missing.\n",
    "    Since `Sales` is our output variable, it makes sense to drop rows missing it.\n",
    "1) Make a transformation to the `Date` column to mimic the paper.\n",
    "1) Merge in the `State` column from a separate sheet.\n",
    "    Initially it's in a textual form, encode it into numeric values.\n",
    "1) Make all the categorical variables start from 0.\n",
    "    It's a requirement to create embeddings correctly, and although this can be done at a later stage, we find it convenient to do it here.\n",
    "1) Rename and reorder the columns, for no reason really."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle\n",
    "\n",
    "dataset = pd.read_csv(\"rossmann-store-sales/train.csv\", usecols=[\"Store\", \"DayOfWeek\", \"Date\", \"Sales\", \"Promo\"])\n",
    "\n",
    "# Drop rows for which we don't have sales figures\n",
    "dataset = dataset[(dataset[\"Sales\"] != 0) & (dataset[\"Sales\"].notna())]\n",
    "\n",
    "# Split Date into Day, Month, Year\n",
    "dataset[[\"Year\", \"Month\", \"Day\"]] = dataset[\"Date\"].str.split(\"-\", expand=True).astype(int)\n",
    "dataset.drop(columns=[\"Date\"], inplace=True)\n",
    "\n",
    "# Add state column\n",
    "state_df = pd.read_csv(\"rossmann-store-sales/store_states.csv\")\n",
    "dataset = pd.merge(dataset, state_df, how=\"left\", on=\"Store\")\n",
    "del state_df\n",
    "\n",
    "# Encode state as integers\n",
    "label_encoder = LabelEncoder()\n",
    "dataset[\"State\"] = label_encoder.fit_transform(dataset[\"State\"])\n",
    "\n",
    "# Shift the range of all categorical variables to start from 0.\n",
    "# This is needed to create embeddings and convenient to do here\n",
    "dataset[dataset.columns.difference(['Sales'])] -= dataset[dataset.columns.difference(['Sales'])].min()\n",
    "\n",
    "# Rename and reorder columns\n",
    "dataset.columns = dataset.columns.str.lower()\n",
    "dataset.rename(columns={\"dayofweek\": \"day_of_week\", \"promo\": \"promotion\"}, inplace=True)\n",
    "dataset = dataset[[\"store\", \"day_of_week\", \"day\", \"month\", \"year\", \"promotion\", \"state\", \"sales\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Tensors\n",
    "\n",
    "Mostly straighforward. We highlight the notable bits.\n",
    "\n",
    "Firsly, the paper describes a transformation to make to the output variable so it is within the range of the sigmoid function. This normalization is done because the Sales column spans 4 orders of magnitudes, and hence doing so allows us to scale to the same range as\n",
    "the neural network output\n",
    "We create a `OutputEncoder` class to encapsulate this so it is easy to do this transformation in both directions.\n",
    "\n",
    "Next we do the test/train split. The paper describes two ways to do it:\n",
    "1) Preseving original temporal ordering \n",
    "    * Since, this way, the test data is of a future time whose probability distrubition has not yet been sampled by the model, it is a better predictor of the model's generalizabiltiy\n",
    "2) Shuffling the data \n",
    "    * This is beneficial for benchmarking model's performance based on its statistical prediction accuracy\n",
    "\n",
    "We implement both and state both results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "X = torch.tensor(dataset.drop(columns=[\"sales\"]).values, dtype=torch.int64)\n",
    "y = torch.tensor(dataset[\"sales\"].values, dtype=torch.float) # create the output tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputEncoder(): # this function just normalizes the output. \n",
    "    def __init__(self, max_output):\n",
    "        self.max_output = max_output\n",
    "\n",
    "    def encode(self, output):\n",
    "        with torch.no_grad():\n",
    "            return torch.log(output) / torch.log(self.max_output)\n",
    "\n",
    "    def decode(self, output):\n",
    "        with torch.no_grad():\n",
    "            return torch.exp(output * torch.log(self.max_output))\n",
    "\n",
    "output_encoder = OutputEncoder(torch.max(y))\n",
    "y = output_encoder.encode(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common split function\n",
    "def test_train_split(X, y):\n",
    "    split_threshold = int(0.9 * X.size(0))\n",
    "    \n",
    "    X_train = X[:split_threshold]\n",
    "    X_test = X[split_threshold:]\n",
    "\n",
    "    y_train = y[:split_threshold]\n",
    "    y_test = y[split_threshold:]\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "# Temporal split (Already in correct order)\n",
    "X_temporal = X.clone()\n",
    "y_temporal = y.clone()\n",
    "\n",
    "# Shuffled split\n",
    "shuffled_indices = torch.randperm(X.size(0))\n",
    "\n",
    "X_shuffled = X[shuffled_indices].clone()\n",
    "y_shuffled = y[shuffled_indices].clone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Neural Networks\n",
    "\n",
    "The architecture is defined well in the paper and we will follow it. The paper defines 2 ways to create the networks, with embeddings and with one-hot vectors. We again implement both ways.\n",
    "\n",
    "* One-hot encoding NN\n",
    "    * The NN with One-hot encodings creates one hot-encoded vectors for the inputs and feeds this into the model. It does not learn any intrinsic propertie/meanings for the features and only learns the output feature (Sales) distribuition based on the input features.\n",
    "* Entity Embeddings NN\n",
    "    * The NN with entity embeddings learns the embedding representation of each categorical feature. This means the model also learns the intrinsic properties/meanings of each feature along with the sales distribuition. This is beneficial when the model sees new data and is able to generalize to it better. It also means that the NN through entitiy emeddings restricts itself in a much smaller but meaningful parameter space. This reduces the chance that the network converges to local minimums far from the global minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EmbeddingNN, self).__init__()\n",
    "        # From TABLE 1. Each tuple is (unique_values, embedding_dimension)\n",
    "        # also includes the embedding layer for Promo which is a binary feature\n",
    "        # the embedding dimensions are hyperparameters defined in the paper. We have used the same values here. \n",
    "        ee_dims = [(1115, 10), (7, 6), (31, 10), (12, 6), (3, 2), (2, 1), (12, 6)]\n",
    "        total_emb_dim = sum(dim for _, dim in ee_dims)\n",
    "\n",
    "        self.ee_layers = [torch.nn.Embedding(*args) for args in ee_dims] # this is the crux of the paper. This is where the NN creates and learns the entity embeddings for the categorical features\n",
    "        self.fc1 = torch.nn.Linear(total_emb_dim, 1000) # first dense layer with dimensions 1000 as defined in the paper\n",
    "        self.relu1 = torch.nn.ReLU() # activation function as defined in the paper\n",
    "        self.fc2 = torch.nn.Linear(1000, 500)  # second dense layer with dimensions 500 as defined in the paper\n",
    "        self.relu2 = torch.nn.ReLU()\n",
    "        self.output = torch.nn.Linear(500, 1)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, X):\n",
    "        embedded = [ee_layer(X[:, i]) for i, ee_layer in enumerate(self.ee_layers)] \n",
    "        embedded = torch.cat(embedded, dim=1) # After we use entity embeddings to represent all categorical variables, all embedding layers and the input of all continuous variables (if any) are concatenated. We did not have continuous variables in our dataset. This concatenated layer of embeddings is used like a normal input layer in neural networks and other dense layers can be build on top of it. \n",
    "        \n",
    "        # The rest of the network is a simple feed-forward neural network with 2 dense layers and a sigmoid activation function at the end. \n",
    "\n",
    "        out = self.relu1(self.fc1(embedded)) \n",
    "        out = self.relu2(self.fc2(out))\n",
    "        out = self.sigmoid(self.output(out)) # In this way, the entity embedding layer learns about the intrinsic properties of each category, while the deeper layers form complex combinations of them\n",
    "        return embedded, out\n",
    "\n",
    "class OneHotNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OneHotNN, self).__init__()\n",
    "        # Required to create the one-hot vectors\n",
    "        self.one_hot_classes = [1115, 7, 31, 12, 3, 2, 12] # the number of unique values for each feature\n",
    "        self.total_classes = sum(self.one_hot_classes)\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(self.total_classes, 1000)\n",
    "        self.relu1 = torch.nn.ReLU()\n",
    "        self.fc2 = torch.nn.Linear(1000, 500)\n",
    "        self.relu2 = torch.nn.ReLU()\n",
    "        self.output = torch.nn.Linear(500, 1)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, X):\n",
    "        one_hot = [torch.nn.functional.one_hot(X[:, i], num_class).float() for i, num_class in enumerate(self.one_hot_classes)]\n",
    "        one_hot = torch.cat(one_hot, dim=1)\n",
    "\n",
    "        out = self.relu1(self.fc1(one_hot))\n",
    "        out = self.relu2(self.fc2(out))\n",
    "        out = self.sigmoid(self.output(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Testing\n",
    "\n",
    "We create some functions to reduce repetitive code when creating multiple models and training on different data.\n",
    "Important things to note are that for predictions, 5 models are created and trained, then their predictions averaged, as mentioned in the paper.\n",
    "The `MAPE` (mean absolute percent error) metric is used for scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(model, X, y):\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    epochs = 10\n",
    "    batch_size = 1024\n",
    "    total_samples = len(X) # we train over the entire dataset in batches of 1024\n",
    "\n",
    "    model.train()\n",
    "    embeddings = None\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(0, total_samples, batch_size):\n",
    "            inputs = X[i:i+batch_size]\n",
    "            targets = y[i:i+batch_size]\n",
    "            \n",
    "            optim.zero_grad()\n",
    "\n",
    "            embeddings, outputs = model(inputs).squeeze()\n",
    "            loss = loss_fn(outputs, targets)\n",
    "        \n",
    "            loss.backward()\n",
    "\n",
    "            optim.step()\n",
    "\n",
    "def MAPE(models, X, y_true): # mean absolute percentage error. This is used since it is more stable with outliers. This is important because we may have outliers in our data which our introduced by the fact that we dropped many columns (features).\n",
    "    y_preds = [model(X).squeeze() for model in models]\n",
    "    stacked_preds = torch.stack(y_preds)\n",
    "    y_pred = torch.mean(stacked_preds, dim=0)\n",
    "\n",
    "    y_pred = output_encoder.decode(y_pred)\n",
    "    y_true = output_encoder.decode(y_true)\n",
    "\n",
    "    return torch.mean(torch.abs((y_true - y_pred) / y_true))\n",
    "\n",
    "def evaluate(cls, X, y):\n",
    "    X_train, y_train, X_test, y_test = test_train_split(X, y)\n",
    "    models = [cls() for _ in range(5)]\n",
    "\n",
    "    for model in models:\n",
    "        train_model(model, X_train, y_train)\n",
    "\n",
    "\n",
    "    return MAPE(models, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffled OneHotNN: 0.070\n",
      "Shuffled EmbeddingNN: 0.080\n",
      "Temporal OneHotNN: 0.440\n",
      "Temporal EmbeddingNN: 0.295\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shuffled OneHotNN: {evaluate(OneHotNN, X_shuffled, y_shuffled):.3f}\")\n",
    "print(f\"Shuffled EmbeddingNN: {evaluate(EmbeddingNN, X_shuffled, y_shuffled):.3f}\")\n",
    "print(f\"Temporal OneHotNN: {evaluate(OneHotNN, X_temporal, y_temporal):.3f}\")\n",
    "print(f\"Temporal EmbeddingNN: {evaluate(EmbeddingNN, X_temporal, y_temporal):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffled RandomForest: 0.109\n",
      "Temporal RandomForest: 0.175\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "def MAPEForest(y_preds, y_true): # mean absolute percentage error. This is used since it is more stable with outliers. This is important because we may have outliers in our data which our introduced by the fact that we dropped many columns (features).\n",
    "    y_pred = torch.tensor(y_preds)\n",
    "    y_pred = output_encoder.decode(y_pred)\n",
    "    y_true = output_encoder.decode(y_true)\n",
    "    return torch.mean(torch.abs((y_true - y_pred) / y_true))\n",
    "\n",
    "def evaluate_sklearn(X, y):\n",
    "    X_train, y_train, X_test, y_test = test_train_split(X, y)\n",
    "    model = RandomForestRegressor()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    return MAPEForest(y_pred, y_test)\n",
    "\n",
    "print(f\"Shuffled RandomForest: {evaluate_sklearn(X_shuffled, y_shuffled):.3f}\")\n",
    "print(f\"Temporal RandomForest: {evaluate_sklearn(X_temporal, y_temporal):.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffled KNN: 0.168\n",
      "Temporal KNN: 0.257\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "def evaluate_knn(X, y):\n",
    "    X_train, y_train, X_test, y_test = test_train_split(X, y)\n",
    "    model = KNeighborsRegressor()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    return MAPEForest(y_pred, y_test)\n",
    "\n",
    "print(f\"Shuffled KNN: {evaluate_knn(X_shuffled, y_shuffled):.3f}\")\n",
    "print(f\"Temporal KNN: {evaluate_knn(X_temporal, y_temporal):.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "We get the following results. Note that the numbers are `MAPE` score.\n",
    "\n",
    "|  | OneHotNN | EmbeddingNN |\n",
    "| --- | --- | --- |\n",
    "| Shuffled Data | 0.075 | 0.082 |\n",
    "| Temporal Data | 0.122 | 0.185 |\n",
    "\n",
    "Compare with the paper's results\n",
    "\n",
    "|  | OneHotNN | EmbeddingNN |\n",
    "| --- | --- | --- |\n",
    "| Shuffled Data | 0.070 | 0.070 |\n",
    "| Temporal Data | 0.101 | 0.093 |\n",
    "\n",
    "Our results are worse than theirs. At worst, twice as bad. It's not immediately clear why, we tried to implement their methodology faithfully."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
