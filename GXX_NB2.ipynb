{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improvement\n",
    "\n",
    "In this notebook we attempt to improve the paper.  Our first strategy is to\n",
    "train the models for the full dataset rather than 200k records, let's see how\n",
    "that goes. The code that follows is copied from the previous notebook with only\n",
    "change being that we did not sample 200k records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffled OneHotNN: 0.065\n",
      "Shuffled EmbeddingNN: 0.072\n",
      "Temporal OneHotNN: 0.094\n",
      "Temporal EmbeddingNN: 0.106\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "relevant_columns = [\"Store\", \"DayOfWeek\", \"Date\", \"Sales\", \"Promo\"]\n",
    "dataset = pd.read_csv(\"rossmann-store-sales/train.csv\", usecols=relevant_columns)\n",
    "\n",
    "dataset = dataset[dataset[\"Sales\"] != 0]\n",
    "\n",
    "dataset[[\"Year\", \"Month\", \"Day\"]] = dataset[\"Date\"].str.split(\"-\", expand=True)\n",
    "dataset.drop(columns=[\"Date\"], inplace=True)\n",
    "\n",
    "state_df = pd.read_csv(\"rossmann-store-sales/store_states.csv\")\n",
    "dataset = pd.merge(dataset, state_df, how=\"left\", on=\"Store\")\n",
    "del state_df\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "for col in dataset.columns.difference([\"Sales\"]):\n",
    "    dataset[col] = label_encoder.fit_transform(dataset[col])\n",
    "\n",
    "dataset = dataset[[\"Store\", \"DayOfWeek\", \"Day\", \"Month\", \"Year\", \"Promo\", \"State\", \"Sales\"]]\n",
    "\n",
    "\n",
    "shuffled_set = dataset.sample(frac=1)\n",
    "shuffled_set = torch.tensor(shuffled_set.values, dtype=torch.float)\n",
    "\n",
    "temporal_set = dataset.iloc[::-1].copy()\n",
    "temporal_set = torch.tensor(temporal_set.values, dtype=torch.float)\n",
    "\n",
    "del dataset\n",
    "\n",
    "class OutputEncoder():\n",
    "    def __init__(self, max_output):\n",
    "        self.max_output = max_output\n",
    "\n",
    "    def encode(self, output):\n",
    "        with torch.no_grad():\n",
    "            return torch.log(output) / torch.log(self.max_output)\n",
    "\n",
    "    def decode(self, output):\n",
    "        with torch.no_grad():\n",
    "            return torch.exp(output * torch.log(self.max_output))\n",
    "\n",
    "output_encoder = OutputEncoder(torch.max(temporal_set[:, -1]))\n",
    "\n",
    "temporal_set[:, -1] = output_encoder.encode(temporal_set[:, -1])\n",
    "shuffled_set[:, -1] = output_encoder.encode(shuffled_set[:, -1])\n",
    "\n",
    "def test_train_split(dataset):\n",
    "    split_threshold = int(0.9 * dataset.size(0))\n",
    "\n",
    "    X_train = dataset[:split_threshold, :-1].long()\n",
    "    X_test = dataset[split_threshold:, :-1].long()\n",
    "\n",
    "    y_train = dataset[:split_threshold, -1]\n",
    "    y_test = dataset[split_threshold:, -1]\n",
    "\n",
    "    # NOTE: This is changed from previous notebook\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    \"store\": (1115, 10),\n",
    "    \"day_of_week\": (7, 6),\n",
    "    \"day\": (31, 10),\n",
    "    \"month\": (12, 6),\n",
    "    \"year\": (3, 2),\n",
    "    \"promotion\": (2, 1),\n",
    "    \"state\": (12, 6)\n",
    "}\n",
    "    \n",
    "class EmbeddingNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EmbeddingNN, self).__init__()\n",
    "\n",
    "        emb_list = [torch.nn.Embedding(n, d) for n, d in parameters.values()]\n",
    "        self.emb_layers = torch.nn.ModuleList(emb_list)\n",
    "\n",
    "        input_size = sum([tuple[1] for tuple in parameters.values()])\n",
    "\n",
    "        self.feed_forward = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_size, 1000),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(1000, 500),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(500, 1),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        embeddings = torch.cat([emb(X[:, i]) for i, emb in enumerate(self.emb_layers)], dim=1)\n",
    "        \n",
    "        return self.feed_forward(embeddings)\n",
    "\n",
    "class OneHotNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OneHotNN, self).__init__()\n",
    "        input_size = sum([tuple[0] for tuple in parameters.values()])\n",
    "\n",
    "        self.feed_forward = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_size, 1000),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(1000, 500),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(500, 1),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        one_hot = torch.cat([torch.nn.functional.one_hot(X[:, i], num_emb).float()\n",
    "                             for i, (num_emb, _) in enumerate(parameters.values())], dim=1)\n",
    "\n",
    "        return self.feed_forward(one_hot)\n",
    "\n",
    "def train_model(model, X, y):\n",
    "    loss_fn = torch.nn.L1Loss()\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    epochs = 10\n",
    "    batch_size = 128\n",
    "    total_samples = len(X)\n",
    "\n",
    "    model.train()\n",
    "    for _ in range(epochs):\n",
    "        for i in range(0, total_samples, batch_size):\n",
    "            inputs = X[i:i+batch_size]\n",
    "            targets = y[i:i+batch_size]\n",
    "            \n",
    "            optim.zero_grad()\n",
    "            outputs = model(inputs).squeeze()\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "def MAPE(y_pred, y_true):\n",
    "    return torch.mean(torch.abs((y_true - y_pred) / y_true))\n",
    "\n",
    "def evaluate(cls, dataset):\n",
    "    X_train, y_train, X_test, y_test = test_train_split(dataset)\n",
    "\n",
    "    models = [cls() for _ in range(5)]\n",
    "    for model in models:\n",
    "        train_model(model, X_train, y_train)\n",
    "\n",
    "    y_preds = []\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "        y_pred = model(X_test).squeeze()\n",
    "        y_pred = output_encoder.decode(y_pred)\n",
    "        y_preds.append(y_pred)\n",
    "\n",
    "    stacked_preds = torch.stack(y_preds)\n",
    "    y_pred = torch.mean(stacked_preds, dim=0)\n",
    "\n",
    "    y_true = output_encoder.decode(y_test)\n",
    "    return MAPE(y_pred, y_true)\n",
    "\n",
    "print(f\"Shuffled OneHotNN: {evaluate(OneHotNN, shuffled_set):.3f}\")\n",
    "print(f\"Shuffled EmbeddingNN: {evaluate(EmbeddingNN, shuffled_set):.3f}\")\n",
    "print(f\"Temporal OneHotNN: {evaluate(OneHotNN, temporal_set):.3f}\")\n",
    "print(f\"Temporal EmbeddingNN: {evaluate(EmbeddingNN, temporal_set):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see improvements to all the results except temporal with embeddings. The\n",
    "reason can be that with temporal data, the test and train test are fundamentally\n",
    "different and adding more training data makes the model overfit and reduces\n",
    "generalizability.\n",
    "\n",
    "Let's try to add more columns. Looking at all options, `Customers` seems like\n",
    "the best choice, since intuitively it makes sense that it would be a strong\n",
    "predictor of `Sales`. We also do feature scaling on this column. Because its\n",
    "range was very large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffled OneHotNN: 0.062\n",
      "Shuffled EmbeddingNN: 0.067\n",
      "Temporal OneHotNN: 0.088\n",
      "Temporal EmbeddingNN: 0.104\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Add Customers column\n",
    "relevant_columns = [\"Store\", \"DayOfWeek\", \"Date\", \"Sales\", \"Promo\", \"Customers\"]\n",
    "dataset = pd.read_csv(\"rossmann-store-sales/train.csv\", usecols=relevant_columns)\n",
    "\n",
    "# Scale Customers by z-score\n",
    "scaler = StandardScaler()\n",
    "dataset[\"Customers\"] = scaler.fit_transform(dataset[[\"Customers\"]])\n",
    "\n",
    "dataset = dataset[dataset[\"Sales\"] != 0]\n",
    "\n",
    "dataset[[\"Year\", \"Month\", \"Day\"]] = dataset[\"Date\"].str.split(\"-\", expand=True)\n",
    "dataset.drop(columns=[\"Date\"], inplace=True)\n",
    "\n",
    "state_df = pd.read_csv(\"rossmann-store-sales/store_states.csv\")\n",
    "dataset = pd.merge(dataset, state_df, how=\"left\", on=\"Store\")\n",
    "del state_df\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "# Customers column is not categorical\n",
    "for col in dataset.columns.difference([\"Sales\", \"Customers\"]):\n",
    "    dataset[col] = label_encoder.fit_transform(dataset[col])\n",
    "\n",
    "dataset = dataset[[\"Store\", \"DayOfWeek\", \"Day\", \"Month\", \"Year\", \"Promo\", \"State\", \"Customers\", \"Sales\"]]\n",
    "\n",
    "\n",
    "shuffled_set = dataset.sample(frac=1)\n",
    "shuffled_set = torch.tensor(shuffled_set.values, dtype=torch.float)\n",
    "\n",
    "temporal_set = dataset.iloc[::-1].copy()\n",
    "temporal_set = torch.tensor(temporal_set.values, dtype=torch.float)\n",
    "\n",
    "del dataset\n",
    "\n",
    "output_encoder = OutputEncoder(torch.max(temporal_set[:, -1]))\n",
    "\n",
    "temporal_set[:, -1] = output_encoder.encode(temporal_set[:, -1])\n",
    "shuffled_set[:, -1] = output_encoder.encode(shuffled_set[:, -1])\n",
    "\n",
    "class EmbeddingNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EmbeddingNN, self).__init__()\n",
    "\n",
    "        emb_list = [torch.nn.Embedding(n, d) for n, d in parameters.values()]\n",
    "        self.emb_layers = torch.nn.ModuleList(emb_list)\n",
    "\n",
    "        # Increase input size to make space for Customers feature\n",
    "        input_size = sum([tuple[1] for tuple in parameters.values()]) + 1\n",
    "\n",
    "        self.feed_forward = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_size, 1000),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(1000, 500),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(500, 1),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        embeddings = torch.cat([emb(X[:, i]) for i, emb in enumerate(self.emb_layers)], dim=1)\n",
    "\n",
    "        # Append Customers column\n",
    "        embeddings = torch.cat((embeddings, X[:, -1].unsqueeze(1)), dim=1)\n",
    "        \n",
    "        return self.feed_forward(embeddings)\n",
    "\n",
    "class OneHotNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OneHotNN, self).__init__()\n",
    "        # Increase input size to make space for Customers feature\n",
    "        input_size = sum([tuple[0] for tuple in parameters.values()]) + 1\n",
    "\n",
    "        self.feed_forward = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_size, 1000),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(1000, 500),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(500, 1),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, X):\n",
    "        one_hot = torch.cat([torch.nn.functional.one_hot(X[:, i], num_emb).float()\n",
    "                             for i, (num_emb, _) in enumerate(parameters.values())], dim=1)\n",
    "        # Append Customers column\n",
    "        one_hot = torch.cat((one_hot, X[:, -1].unsqueeze(1)), dim=1)\n",
    "\n",
    "        return self.feed_forward(one_hot)\n",
    "\n",
    "print(f\"Shuffled OneHotNN: {evaluate(OneHotNN, shuffled_set):.3f}\")\n",
    "print(f\"Shuffled EmbeddingNN: {evaluate(EmbeddingNN, shuffled_set):.3f}\")\n",
    "print(f\"Temporal OneHotNN: {evaluate(OneHotNN, temporal_set):.3f}\")\n",
    "print(f\"Temporal EmbeddingNN: {evaluate(EmbeddingNN, temporal_set):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see improvements over the previous result, although for some categories it\n",
    "still does not beat their score."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
