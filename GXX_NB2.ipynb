{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improvement\n",
    "\n",
    "In this notebook we attempt to improve the results obtained in the paper. Our first strategy is to\n",
    "train the models for the full dataset rather than 200k records, let's see how\n",
    "that goes. The code that follows is copied from the previous notebook with only\n",
    "change being that we did not sample 200k records. This is why we ahve commented it less since all the details for this are already in Notebook 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffled OneHotNN: 0.065\n",
      "Shuffled EmbeddingNN: 0.072\n",
      "Temporal OneHotNN: 0.094\n",
      "Temporal EmbeddingNN: 0.106\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Read relevant columns from the Rossman store sales dataset\n",
    "relevant_columns = [\"Store\", \"DayOfWeek\", \"Date\", \"Sales\", \"Promo\"]\n",
    "dataset = pd.read_csv(\"rossmann-store-sales/train.csv\",\n",
    "                      usecols=relevant_columns)\n",
    "\n",
    "# Filter out records with zero sales\n",
    "dataset = dataset[dataset[\"Sales\"] != 0]\n",
    "\n",
    "# Extract year, month, day from the \"Date\" column\n",
    "dataset[[\"Year\", \"Month\", \"Day\"]] = dataset[\"Date\"].str.split(\"-\", expand=True)\n",
    "dataset.drop(columns=[\"Date\"], inplace=True)\n",
    "\n",
    "# Now merge dataset with store states information\n",
    "state_df = pd.read_csv(\"rossmann-store-sales/store_states.csv\")\n",
    "dataset = pd.merge(dataset, state_df, how=\"left\", on=\"Store\")\n",
    "del state_df\n",
    "\n",
    "# Encode categorical variables using LabelEncoder. This just assigns a number to each category\n",
    "label_encoder = LabelEncoder()\n",
    "for col in dataset.columns.difference([\"Sales\"]): # We don't encode the target variable ofc\n",
    "    dataset[col] = label_encoder.fit_transform(dataset[col])\n",
    "\n",
    "# We select relevant columns and shuffle the dataset\n",
    "dataset = dataset[[\"Store\", \"DayOfWeek\", \"Day\",\n",
    "                   \"Month\", \"Year\", \"Promo\", \"State\", \"Sales\"]]\n",
    "\n",
    "shuffled_set = dataset.sample(frac=1)\n",
    "shuffled_set = torch.tensor(shuffled_set.values, dtype=torch.float) # Convert to tensor for PyTorch training later with float dtype\n",
    "\n",
    "# Create a temporal set for evaluation purposes. This is the same as the dataset but in reverse order as the dataset is in reverse chronological order\n",
    "temporal_set = dataset.iloc[::-1].copy()\n",
    "temporal_set = torch.tensor(temporal_set.values, dtype=torch.float)\n",
    "\n",
    "del dataset # We don't need the dataset anymore\n",
    "\n",
    "# An output encoder for normalization of the target variable. \n",
    "\n",
    "class OutputEncoder():\n",
    "    def __init__(self, max_output):\n",
    "        self.max_output = max_output\n",
    "\n",
    "    def encode(self, output):\n",
    "        with torch.no_grad():\n",
    "            return torch.log(output) / torch.log(self.max_output) # We use log to normalize the output\n",
    "\n",
    "    def decode(self, output):\n",
    "        with torch.no_grad():\n",
    "            return torch.exp(output * torch.log(self.max_output)) # We use exp to reverse the normalization\n",
    "\n",
    "\n",
    "# Normalize the target variable in both sets\n",
    "output_encoder = OutputEncoder(torch.max(temporal_set[:, -1]))\n",
    "temporal_set[:, -1] = output_encoder.encode(temporal_set[:, -1])\n",
    "shuffled_set[:, -1] = output_encoder.encode(shuffled_set[:, -1])\n",
    "\n",
    "# Function for train-test split\n",
    "\n",
    "def test_train_split(dataset):\n",
    "    split_threshold = int(0.9 * dataset.size(0)) # 90% of the dataset is used for training\n",
    "\n",
    "    X_train = dataset[:split_threshold, :-1].long()  # We don't need the target variable for the input. We also convert to long dtype because in PyTorch, embedding layers typically expect input of type torch.long (or torch.int64). This is because embedding layers are designed to work with discrete indices, such as those used to represent categories or words.\n",
    "    X_test = dataset[split_threshold:, :-1].long()\n",
    "\n",
    "    y_train = dataset[:split_threshold, -1] # We only need the target variable for the output\n",
    "    y_test = dataset[split_threshold:, -1]\n",
    "\n",
    "    # NOTE: This is changed from previous notebook. Now we are no longer selecting only 200k rows\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "# Parameters for embedding layers\n",
    "parameters = {\n",
    "    \"store\": (1115, 10),\n",
    "    \"day_of_week\": (7, 6),\n",
    "    \"day\": (31, 10),\n",
    "    \"month\": (12, 6),\n",
    "    \"year\": (3, 2),\n",
    "    \"promotion\": (2, 1),\n",
    "    \"state\": (12, 6)\n",
    "}\n",
    "\n",
    "# Neural network with embedding layers\n",
    "\n",
    "\n",
    "class EmbeddingNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EmbeddingNN, self).__init__()\n",
    "        # Created a list of Embedding layers based on parameters\n",
    "        emb_list = [torch.nn.Embedding(n, d) for n, d in parameters.values()]\n",
    "        # List of Embedding layers converted into a ModuleList\n",
    "        self.emb_layers = torch.nn.ModuleList(emb_list)\n",
    "        # Calculating the total input size for the feed-forward layers\n",
    "        input_size = sum([tuple[1] for tuple in parameters.values()])\n",
    "        # Feed-forward layers as a sequential model\n",
    "        self.feed_forward = torch.nn.Sequential(\n",
    "            # First linear layer: Input size to 1000 units\n",
    "            torch.nn.Linear(input_size, 1000),\n",
    "            # Applied Rectified Linear Unit (ReLU) activation function\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(1000, 500),\n",
    "            # Applied Rectified Linear Unit (ReLU) activation function\n",
    "            torch.nn.ReLU(),\n",
    "            # Third linear layer: 500 units to 1 unit\n",
    "            torch.nn.Linear(500, 1),\n",
    "            # Applied Sigmoid activation function (for binary classification)\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Concatenate the embeddings obtained from each categorical feature using list comprehension\n",
    "        # Iterate over the embedding layers and apply them to corresponding columns in the input X\n",
    "        # The resulting embeddings are concatenated along the specified dimension (dim=1) to form a single tensor\n",
    "        embeddings = torch.cat([emb(X[:, i])\n",
    "                               for i, emb in enumerate(self.emb_layers)], dim=1)\n",
    "        # Now pass the concatenated embeddings through the feed-forward layers\n",
    "        return self.feed_forward(embeddings)\n",
    "\n",
    "# Neural network with one-hot encoding\n",
    "\n",
    "\n",
    "class OneHotNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OneHotNN, self).__init__()\n",
    "\n",
    "        # Calculating the total input size for the feed-forward layers\n",
    "        input_size = sum([tuple[0] for tuple in parameters.values()])\n",
    "\n",
    "        # Feed-forward layers as a sequential mode\n",
    "        self.feed_forward = torch.nn.Sequential(\n",
    "            # First linear layer: Input size to 1000 units\n",
    "            torch.nn.Linear(input_size, 1000),\n",
    "            # Applied Rectified Linear Unit (ReLU) activation function\n",
    "            torch.nn.ReLU(),\n",
    "            # Second linear layer: 1000 units to 500 units\n",
    "            torch.nn.Linear(1000, 500),\n",
    "            # Applied Rectified Linear Unit (ReLU) activation function\n",
    "            torch.nn.ReLU(),\n",
    "            # Third linear layer: 500 units to 1 unit\n",
    "            torch.nn.Linear(500, 1),\n",
    "            # Applied Sigmoid activation function (for binary classification)\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Converting categorical features into one-hot encoding\n",
    "        one_hot = torch.cat([torch.nn.functional.one_hot(X[:, i], num_emb).float()\n",
    "                             for i, (num_emb, _) in enumerate(parameters.values())], dim=1)\n",
    "        # Now pass the one-hot encoded features through the feed-forward layers\n",
    "        return self.feed_forward(one_hot)\n",
    "\n",
    "# Model training function\n",
    "\n",
    "\n",
    "def train_model(model, X, y):\n",
    "    # Loss function: Mean Absolute Error nd optimizer: Adam\n",
    "    loss_fn = torch.nn.L1Loss()\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    epochs = 10\n",
    "    batch_size = 128\n",
    "    total_samples = len(X)\n",
    "\n",
    "    model.train()\n",
    "    for _ in range(epochs):\n",
    "        # Iterating over the dataset in mini-batches\n",
    "        for i in range(0, total_samples, batch_size):\n",
    "            inputs = X[i:i+batch_size]\n",
    "            targets = y[i:i+batch_size]\n",
    "            # Zero the gradients, this clears previous gradients\n",
    "            optim.zero_grad()\n",
    "            # Forward pass: computes predicted outputs by passing inputs to the model\n",
    "            outputs = model(inputs).squeeze()\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            # Backward pass: calculates gradient of the loss with respect to the model parameters\n",
    "            loss.backward()\n",
    "            # Update model parameters using the optimizer\n",
    "            optim.step()\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE)\n",
    "\n",
    "\n",
    "def MAPE(y_pred, y_true):\n",
    "    return torch.mean(torch.abs((y_true - y_pred) / y_true))\n",
    "\n",
    "# Model evaluation\n",
    "\n",
    "def evaluate(cls, dataset):\n",
    "    X_train, y_train, X_test, y_test = test_train_split(dataset)\n",
    "\n",
    "    # Create and train multiple models for ensemble learning\n",
    "    models = [cls() for _ in range(5)]\n",
    "    for model in models:\n",
    "        train_model(model, X_train, y_train)\n",
    "\n",
    "    # Evaluate each model on the test set and store predictions\n",
    "    y_preds = []\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "        # Get the predictions and decode them using the output encoder\n",
    "        y_pred = model(X_test).squeeze()\n",
    "        y_pred = output_encoder.decode(y_pred)\n",
    "        y_preds.append(y_pred)\n",
    "    # Predictions are stacked and then compute the mean prediction\n",
    "    stacked_preds = torch.stack(y_preds)\n",
    "    y_pred = torch.mean(stacked_preds, dim=0)\n",
    "    # Decode true values using the output encoder\n",
    "    y_true = output_encoder.decode(y_test)\n",
    "    # Return the MAPE score for the ensemble predictions\n",
    "    return MAPE(y_pred, y_true)\n",
    "\n",
    "\n",
    "print(f\"Shuffled OneHotNN: {evaluate(OneHotNN, shuffled_set):.3f}\")\n",
    "print(f\"Shuffled EmbeddingNN: {evaluate(EmbeddingNN, shuffled_set):.3f}\")\n",
    "print(f\"Temporal OneHotNN: {evaluate(OneHotNN, temporal_set):.3f}\")\n",
    "print(f\"Temporal EmbeddingNN: {evaluate(EmbeddingNN, temporal_set):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtained the following results, with the numbers representing the `MAPE` scores.\n",
    "\n",
    "|  | OneHotNN | EmbeddingNN |\n",
    "| --- | --- | --- |\n",
    "| Shuffled Data | 0.065 | 0.072 |\n",
    "| Temporal Data | 0.094 | 0.106 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see improvements to all the results except temporal with embeddings. The\n",
    "reason can be that with temporal data, the test and train test are fundamentally\n",
    "different and adding more training data makes the model overfit and reduces\n",
    "generalizability.\n",
    "\n",
    "Let's try to add more columns. Looking at all options, `Customers` seems like\n",
    "the best choice, since intuitively it makes sense that it would be a strong\n",
    "predictor of `Sales`. We also do feature scaling on this column. Because its\n",
    "range was very large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffled OneHotNN: 0.062\n",
      "Shuffled EmbeddingNN: 0.067\n",
      "Temporal OneHotNN: 0.088\n",
      "Temporal EmbeddingNN: 0.104\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Add Customers column\n",
    "relevant_columns = [\"Store\", \"DayOfWeek\",\n",
    "                    \"Date\", \"Sales\", \"Promo\", \"Customers\"]\n",
    "dataset = pd.read_csv(\"rossmann-store-sales/train.csv\",\n",
    "                      usecols=relevant_columns)\n",
    "\n",
    "# Scale Customers by z-score\n",
    "scaler = StandardScaler()\n",
    "dataset[\"Customers\"] = scaler.fit_transform(dataset[[\"Customers\"]])\n",
    "\n",
    "# Filtered out records with zero sales\n",
    "dataset = dataset[dataset[\"Sales\"] != 0]\n",
    "\n",
    "dataset[[\"Year\", \"Month\", \"Day\"]] = dataset[\"Date\"].str.split(\"-\", expand=True)\n",
    "dataset.drop(columns=[\"Date\"], inplace=True)\n",
    "\n",
    "# Now merge dataset with store states information\n",
    "state_df = pd.read_csv(\"rossmann-store-sales/store_states.csv\")\n",
    "dataset = pd.merge(dataset, state_df, how=\"left\", on=\"Store\")\n",
    "del state_df\n",
    "\n",
    "# Encode categorical variables using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "# Customers column is not categorical\n",
    "for col in dataset.columns.difference([\"Sales\", \"Customers\"]):\n",
    "    dataset[col] = label_encoder.fit_transform(dataset[col])\n",
    "# We select relevant columns and shuffle the dataset\n",
    "dataset = dataset[[\"Store\", \"DayOfWeek\", \"Day\", \"Month\",\n",
    "                   \"Year\", \"Promo\", \"State\", \"Customers\", \"Sales\"]]\n",
    "\n",
    "\n",
    "shuffled_set = dataset.sample(frac=1)\n",
    "shuffled_set = torch.tensor(shuffled_set.values, dtype=torch.float)\n",
    "\n",
    "# Create a temporal set for evaluation purposes\n",
    "temporal_set = dataset.iloc[::-1].copy()\n",
    "temporal_set = torch.tensor(temporal_set.values, dtype=torch.float)\n",
    "# Memory clean up\n",
    "del dataset\n",
    "\n",
    "# Created an output encoder for normalization of the target variable\n",
    "output_encoder = OutputEncoder(torch.max(temporal_set[:, -1]))\n",
    "\n",
    "# Normalize the target variable in both sets\n",
    "temporal_set[:, -1] = output_encoder.encode(temporal_set[:, -1])\n",
    "shuffled_set[:, -1] = output_encoder.encode(shuffled_set[:, -1])\n",
    "\n",
    "# Neural network with embedding layers\n",
    "\n",
    "class EmbeddingNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EmbeddingNN, self).__init__()\n",
    "        # Created a list of Embedding layers based on parameters\n",
    "        emb_list = [torch.nn.Embedding(n, d) for n, d in parameters.values()]\n",
    "        # List of Embedding layers converted into a ModuleList\n",
    "        self.emb_layers = torch.nn.ModuleList(emb_list)\n",
    "\n",
    "        # Increase input size to make space for Customers feature. Importantly, we are not using an embedding layer for Customers\n",
    "        input_size = sum([tuple[1] for tuple in parameters.values()]) + 1\n",
    "\n",
    "        # this part is same as before\n",
    "\n",
    "        self.feed_forward = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_size, 1000),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(1000, 500),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(500, 1),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Concatenate the embeddings obtained from each categorical feature using list comprehension\n",
    "        # Iterate over the embedding layers and apply them to corresponding columns in the input X\n",
    "        # The resulting embeddings are concatenated along the specified dimension (dim=1) to form a single tensor\n",
    "        embeddings = torch.cat([emb(X[:, i])\n",
    "                               for i, emb in enumerate(self.emb_layers)], dim=1)\n",
    "\n",
    "        # Append Customers column\n",
    "        embeddings = torch.cat((embeddings, X[:, -1].unsqueeze(1)), dim=1)\n",
    "\n",
    "        return self.feed_forward(embeddings)\n",
    "\n",
    "\n",
    "class OneHotNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OneHotNN, self).__init__()\n",
    "        # Increase input size to make space for Customers feature\n",
    "        input_size = sum([tuple[0] for tuple in parameters.values()]) + 1\n",
    "        # Feed-forward layers as a sequential model\n",
    "        self.feed_forward = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_size, 1000),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(1000, 500),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(500, 1),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Convert categorical features into one-hot encoding\n",
    "        one_hot = torch.cat([torch.nn.functional.one_hot(X[:, i], num_emb).float()\n",
    "                             for i, (num_emb, _) in enumerate(parameters.values())], dim=1)\n",
    "        # Append Customers column\n",
    "        one_hot = torch.cat((one_hot, X[:, -1].unsqueeze(1)), dim=1)\n",
    "\n",
    "        return self.feed_forward(one_hot)\n",
    "\n",
    "\n",
    "print(f\"Shuffled OneHotNN: {evaluate(OneHotNN, shuffled_set):.3f}\")\n",
    "print(f\"Shuffled EmbeddingNN: {evaluate(EmbeddingNN, shuffled_set):.3f}\")\n",
    "print(f\"Temporal OneHotNN: {evaluate(OneHotNN, temporal_set):.3f}\")\n",
    "print(f\"Temporal EmbeddingNN: {evaluate(EmbeddingNN, temporal_set):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtained the following results, with the numbers representing the `MAPE` scores.\n",
    "\n",
    "|  | OneHotNN | EmbeddingNN |\n",
    "| --- | --- | --- |\n",
    "| Shuffled Data | 0.062 | 0.067 |\n",
    "| Temporal Data | 0.088 | 0.104 |\n",
    "\n",
    "### Comparison with Paper's Results\n",
    "\n",
    "|  | OneHotNN | EmbeddingNN |\n",
    "| --- | --- | --- |\n",
    "| Shuffled Data | 0.070 | 0.070 |\n",
    "| Temporal Data | 0.101 | 0.093 |\n",
    "\n",
    "\n",
    "We see improvements over the previous result, although for some categories it\n",
    "still does not beat their score. The breakdown of all this is explained in our Methodology PDF."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
