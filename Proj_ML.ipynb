{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn import preprocessing\n",
    "import random\n",
    "# torch.set_num_threads(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the store df: (1115, 10)\n",
      "The column names before dropping are : ['Store', 'DayOfWeek', 'Sales', 'Customers', 'Open', 'Promo', 'StateHoliday', 'SchoolHoliday', 'StoreType', 'Assortment', 'CompetitionDistance', 'CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear', 'Promo2', 'Promo2SinceWeek', 'Promo2SinceYear', 'PromoInterval', 'Year', 'Month', 'Day', 'State']\n",
      "Size of the train dataset after merging is : (1017209, 21)\n",
      "object\n",
      "Size of the train dataset after merging and preprocessing is : (844338, 9)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "file_path = ['rossmann-store-sales/train.csv',\n",
    "             'rossmann-store-sales/test.csv', 'rossmann-store-sales/store.csv', 'rossmann-store-sales/store_states.csv']\n",
    "store_df = pd.read_csv(file_path[2], low_memory=False, dtype=str)\n",
    "store_states_df = pd.read_csv(file_path[3], low_memory=False,  dtype=str)\n",
    "\n",
    "print(f\"Size of the store df: {store_df.shape}\")\n",
    "\n",
    "train_df = pd.read_csv(file_path[0], low_memory=False, dtype=str) # split this into test train for validation ++ we may also need to reverse the order of the data\n",
    "test_df = pd.read_csv(file_path[1], low_memory=False, dtype=str)\n",
    "train_df = pd.merge(train_df, store_df, how=\"inner\", on=\"Store\") #  one thing to note is that when they pre process data, they keep the store and test data separate, \n",
    "test_df = pd.merge(test_df, store_df, how=\"inner\", on=\"Store\")\n",
    "train_df['Date'] = pd.to_datetime(train_df['Date'])\n",
    "train_df['Year'], train_df['Month'], train_df['Day'] = train_df['Date'].dt.year.values, train_df['Date'].dt.month.values, train_df['Date'].dt.day.values\n",
    "train_df.drop(['Date'], axis=1, inplace=True)\n",
    "train_df = pd.merge(train_df, store_states_df, how=\"inner\", on=\"Store\")\n",
    "test_df = pd.merge(test_df, store_states_df, how=\"inner\", on=\"Store\")\n",
    "\n",
    "print(f\"The column names before dropping are : {train_df.columns.tolist()}\")\n",
    "print(f\"Size of the train dataset after merging is : {train_df.shape}\")\n",
    "# print(f\"Size of the test dataset after merging is : {test_df.shape}\")\n",
    "print(train_df['Store'].dtype)\n",
    "\n",
    "# helper function to replace nan values if any\n",
    "train_df.fillna('0', inplace=True)\n",
    "\n",
    "train_df = train_df[train_df[\"Sales\"]!=\"0\"]\n",
    "train_df = train_df[train_df[\"Open\"]!=\"\"]\n",
    "cols_to_drop = ['StateHoliday', 'SchoolHoliday', 'CompetitionOpenSinceMonth', 'StoreType', 'Assortment', 'PromoInterval', 'Promo2SinceWeek', 'Promo2SinceYear', 'Promo2', 'CompetitionOpenSinceYear', 'CompetitionDistance', 'Customers']\n",
    "\n",
    "train_df.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "train_data_y = pd.DataFrame(train_df['Sales'])\n",
    "train_data_x = train_df.drop(['Sales'], axis=1)\n",
    "\n",
    "for feature in train_data_x.columns: # this is to convert the categorical data into numerical data\n",
    "    label_encoder = preprocessing.LabelEncoder()\n",
    "    train_data_x.loc[:,feature] = label_encoder.fit_transform(train_data_x[feature].astype(str).fillna(\"0\").values)\n",
    "train_data_x = train_data_x.reindex(['Open','Store', 'DayOfWeek',  'Promo', 'Year', 'Month', 'Day' ,'State'], axis=1)\n",
    "# train_data_x = train_data_x.astype(int)\n",
    "# train_data_y = train_data_y.astype(int)\n",
    "print(f\"Size of the train dataset after merging and preprocessing is : {train_df.shape}\")\n",
    "##############################################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>Store</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>Promo</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>State</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>24</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>23</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Open Store DayOfWeek Promo  Year  Month  Day State\n",
       "0     0     0         4     1     2      9   24     4\n",
       "1     0     0         3     1     2      9   23     4\n",
       "2     0     0         2     1     2      9   21     4\n",
       "3     0     0         1     1     2      9   20     4\n",
       "4     0     0         0     1     2      9   19     4\n",
       "6     0     0         5     0     2      9   17     4\n",
       "7     0     0         4     0     2      9   16     4\n",
       "8     0     0         3     0     2      9   15     4\n",
       "9     0     0         2     0     2      9   14     4\n",
       "10    0     0         1     0     2      9   13     4"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_x.head(10) # shows random 10 instead of first ten like head\n",
    "# print(train_data_x.shape)\n",
    "\n",
    "# they kept 8 columns as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(844338, 8)\n",
      "(844338, 1)\n"
     ]
    }
   ],
   "source": [
    "print(train_data_x.shape) # matches\n",
    "print(train_data_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import pickle\n",
    "\n",
    "train_ratio = 0.9\n",
    "shuffle_data = False\n",
    "one_hot_as_input = False\n",
    "embeddings_as_input = False\n",
    "save_embeddings = True\n",
    "saved_embeddings_fname = \"embeddings.pickle\"  # set save_embeddings to True to create this file\n",
    "\n",
    "train_data_x = train_data_x.astype(np.int64)\n",
    "train_data_y = train_data_y.astype(np.int64)\n",
    "\n",
    "def split_features(X): # this function takes a numpy array and splits it into a list of np arrays. It returns a list of all the features/columns in the dataset\n",
    " # Extract the column with index 0 (store) and append to the list\n",
    "    X_list = []\n",
    "    store_index = X[:, 0].reshape(-1, 1)\n",
    "    X_list.append(np.array(store_index, dtype=np.int64))\n",
    "\n",
    "    # Extract the column with index 1 (day_of_week) and append to the list\n",
    "    day_of_week = X[:, 1].reshape(-1, 1)\n",
    "    X_list.append(np.array(day_of_week, dtype=np.int64))\n",
    "\n",
    "    # Extract the column with index 2 (promo) and append to the list\n",
    "    promo = X[:, 2].reshape(-1, 1)\n",
    "    X_list.append(np.array(promo, dtype=np.int64))\n",
    "\n",
    "    # Extract the column with index 3 (year) and append to the list\n",
    "    year = X[:, 3].reshape(-1, 1)\n",
    "    X_list.append(np.array(year, dtype=np.int64))\n",
    "\n",
    "    # Extract the column with index 4 (month) and append to the list\n",
    "    month = X[:, 4].reshape(-1, 1)\n",
    "    X_list.append(np.array(month, dtype=np.int64))\n",
    "\n",
    "    # Extract the column with index 5 (day) and append to the list\n",
    "    day = X[:, 5].reshape(-1, 1)\n",
    "    X_list.append(np.array(day, dtype=np.int64))\n",
    "    # Extract the column with index 6 (state) and append to the list\n",
    "    state = X[:, 6].reshape(-1, 1)\n",
    "    X_list.append(np.array(state, dtype=np.int64))\n",
    "\n",
    "    return X_list\n",
    " \n",
    "split_features(train_data_x.values)\n",
    "\n",
    "\n",
    "# okay they ar enot really suing this/ they only use this to feed embeddings leanred duing NN to other models as input\n",
    "def embed_features(X, saved_embeddings_fname): # this function creates and saves the embeddings for the categorical data\n",
    "   f_embeddings = open(saved_embeddings_fname, \"rb\")\n",
    "   embeddings = pickle.load(f_embeddings)\n",
    "\n",
    "   index_embedding_mapping = {1: 0, 2: 1, 4: 2, 5: 3, 6: 4, 7: 5} # this is the mapping of the columns index to the embeddings index. They skipped columns 1 and 3. Opena and Promo because they are 0, 1? I guess binary categories are useless to embed. Embedding these binary features may not provide significant benefits, as their information is already somewhat captured by the binary nature.\n",
    "   X_embedded = []\n",
    "   print(X.shape)\n",
    "\n",
    "   (num_records, num_features) = X.shape\n",
    "   for record in X: # a record is a row in the dataset\n",
    "      # print(record)\n",
    "      embedded_features = []\n",
    "      for i, feat in enumerate(record): # this is to embed the features\n",
    "         # print(i, feat)\n",
    "         feat = int(feat)\n",
    "         if i not in index_embedding_mapping.keys():\n",
    "               embedded_features += [feat]\n",
    "         else:\n",
    "               embedding_index = index_embedding_mapping[i]\n",
    "               embedded_features += embeddings[embedding_index][feat].tolist()\n",
    "\n",
    "      X_embedded.append(embedded_features)\n",
    "\n",
    "   return pd.DataFrame(X_embedded) # returns the embedded features\n",
    "   # return np.array(X_embedded) # returns the embedded features\n",
    "\n",
    "# j = embed_features((np.array(train_data_x)), saved_embeddings_fname)\n",
    "\n",
    "# print(j)\n",
    "\n",
    "# print(j.shape) # okay shape matches theirs as well\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Sales\n",
      "934085   6760\n",
      "937882   6833\n",
      "977052   8578\n",
      "165948   8467\n",
      "658089   5214\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# train_data_x = j\n",
    "# train_data_y = np.array(train_data_y)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_data_x, train_data_y, test_size=0.1, random_state=42) # this is to split the data into train and test sets\n",
    "print(y_train[:5])\n",
    "for data in [X_train, X_val]:\n",
    "    data.drop(['Open'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So basically there is an entity embedding layer for each categoircal feature (Store, DOW, etc). OPen and Promo are not inlcuded since they are binary. \n",
    "These are the embedding dimesions for different categories used in the paper            \n",
    "cat_emb_dim={\n",
    "                'Store': 10,\n",
    "                'DayOfWeek': 6,\n",
    "                'Promo': 1,\n",
    "                'Year': 2,\n",
    "                'Month': 6,\n",
    "                'Day': 10,\n",
    "                'State': 6},}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Sales\n",
      "934085   6760\n",
      "937882   6833\n",
      "977052   8578\n",
      "165948   8467\n",
      "658089   5214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/loss.py:101: UserWarning: Using a target size (torch.Size([128, 1])) that is different to the input size (torch.Size([128])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/loss.py:101: UserWarning: Using a target size (torch.Size([96, 1])) that is different to the input size (torch.Size([96])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.030744114890694618\n",
      "Epoch 2/10, Loss: 0.030887659639120102\n",
      "Epoch 3/10, Loss: 0.03071647882461548\n",
      "Epoch 4/10, Loss: 0.030838701874017715\n",
      "Epoch 5/10, Loss: 0.03091205097734928\n",
      "Epoch 6/10, Loss: 0.030927734449505806\n",
      "Epoch 7/10, Loss: 0.03092590905725956\n",
      "Epoch 8/10, Loss: 0.030922962352633476\n",
      "Epoch 9/10, Loss: 0.030890056863427162\n",
      "Epoch 10/10, Loss: 0.03087809681892395\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/aisha/Desktop/ML Proj/Proj_ML.ipynb Cell 10\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aisha/Desktop/ML%20Proj/Proj_ML.ipynb#X33sZmlsZQ%3D%3D?line=91'>92</a>\u001b[0m \u001b[39m# Prediction example\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aisha/Desktop/ML%20Proj/Proj_ML.ipynb#X33sZmlsZQ%3D%3D?line=92'>93</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/aisha/Desktop/ML%20Proj/Proj_ML.ipynb#X33sZmlsZQ%3D%3D?line=93'>94</a>\u001b[0m     output \u001b[39m=\u001b[39m pytorch_model(\u001b[39m*\u001b[39m[torch\u001b[39m.\u001b[39mfrom_numpy(example)\u001b[39m.\u001b[39mlong() \u001b[39mfor\u001b[39;00m example \u001b[39min\u001b[39;00m features])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aisha/Desktop/ML%20Proj/Proj_ML.ipynb#X33sZmlsZQ%3D%3D?line=94'>95</a>\u001b[0m     prediction \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aisha/Desktop/ML%20Proj/Proj_ML.ipynb#X33sZmlsZQ%3D%3D?line=95'>96</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mPrediction:\u001b[39m\u001b[39m\"\u001b[39m, prediction)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'features' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PyTorchModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PyTorchModel, self).__init__()\n",
    "        \n",
    "        # Define Embeddings\n",
    "        self.store_embedding = nn.Embedding(1115, 10)\n",
    "        self.dow_embedding = nn.Embedding(7, 6)\n",
    "        self.year_embedding = nn.Embedding(3, 2)\n",
    "        self.month_embedding = nn.Embedding(12, 6)\n",
    "        self.day_embedding = nn.Embedding(31, 10)\n",
    "        self.state_embedding = nn.Embedding(12, 6)\n",
    "        \n",
    "        # Define other layers\n",
    "        self.promo_layer = nn.Linear(1, 1)\n",
    "        self.dense1 = nn.Linear(41, 1000)\n",
    "        self.dense2 = nn.Linear(1000, 500)\n",
    "        self.dense3 = nn.Linear(500, 1)\n",
    "        \n",
    "    def forward(self, input_store, input_dow, input_promo, input_year, input_month, input_day, input_state):\n",
    "        # Apply embeddings\n",
    "        input_promo = input_promo.unsqueeze(1)\n",
    "        output_store = self.store_embedding(input_store.view(-1, 1))\n",
    "        output_dow = self.dow_embedding(input_dow.view(-1, 1))\n",
    "        output_year = self.year_embedding(input_year.view(-1, 1))\n",
    "        output_month = self.month_embedding(input_month.view(-1, 1))\n",
    "        output_day = self.day_embedding(input_day.view(-1, 1))\n",
    "        output_state = self.state_embedding(input_state.view(-1, 1))\n",
    "        \n",
    "        # Reshape embeddings\n",
    "        output_store = output_store.view(-1, 10)\n",
    "        output_dow = output_dow.view(-1, 6)\n",
    "        output_year = output_year.view(-1, 2)\n",
    "        output_month = output_month.view(-1, 6)\n",
    "        output_day = output_day.view(-1, 10)\n",
    "        output_state = output_state.view(-1, 6)\n",
    "        \n",
    "        # print(output_store.shape)\n",
    "        # print(output_dow.shape)\n",
    "        # print(input_promo.shape)\n",
    "                \n",
    "        # Concatenate embeddings\n",
    "        concatenated = torch.cat([output_store, output_dow, input_promo, output_year, output_month, output_day, output_state], dim=1)\n",
    "        # print(concatenated.shape)\n",
    "        # Feed forward through other layers\n",
    "        x = F.relu(self.dense1(concatenated))\n",
    "        x = F.relu(self.dense2(x))\n",
    "        x = self.dense3(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "pytorch_model = PyTorchModel()\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = optim.Adam(pytorch_model.parameters(), lr=0.001)\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "print(y_train[:5])\n",
    "max_log_y = max(np.max(np.log(y_train)), np.max(np.log(y_val))) # this is to normalize the data\n",
    "y_train = np.log(y_train)/max_log_y\n",
    "# print(\"After \\n\",y_train[:5])\n",
    "y_val = np.log(y_val)/max_log_y\n",
    "X_train_tensor = torch.from_numpy(np.array(X_train))\n",
    "y_train_tensor = torch.from_numpy(np.array(y_train))\n",
    "\n",
    "# Training loop\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        batch_X = X_train_tensor[i:i+batch_size]\n",
    "        batch_y = y_train_tensor[i:i+batch_size]\n",
    "        # print(batch_X.shape)\n",
    "        # print(batch_y.shape)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = pytorch_model(batch_X[:, 0], batch_X[:, 1], batch_X[:, 2], batch_X[:, 3], batch_X[:, 4], batch_X[:, 5], batch_X[:, 6])\n",
    "\n",
    "        loss = criterion(output.flatten(), batch_y.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss.item()}')\n",
    "\n",
    "# # Prediction example\n",
    "# with torch.no_grad():\n",
    "#     output = pytorch_model(*[torch.from_numpy(example).long() for example in features])\n",
    "#     prediction = output.numpy()\n",
    "#     print(\"Prediction:\", prediction)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
