{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn import preprocessing\n",
    "import random\n",
    "# torch.set_num_threads(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the store df: (1115, 10)\n",
      "The column names before dropping are : ['Store', 'DayOfWeek', 'Sales', 'Customers', 'Open', 'Promo', 'StateHoliday', 'SchoolHoliday', 'StoreType', 'Assortment', 'CompetitionDistance', 'CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear', 'Promo2', 'Promo2SinceWeek', 'Promo2SinceYear', 'PromoInterval', 'Year', 'Month', 'Day', 'State']\n",
      "Size of the train dataset after merging is : (1017209, 21)\n",
      "object\n",
      "Size of the train dataset after merging and preprocessing is : (844338, 9)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "file_path = ['rossmann-store-sales/train.csv',\n",
    "             'rossmann-store-sales/test.csv', 'rossmann-store-sales/store.csv', 'rossmann-store-sales/store_states.csv']\n",
    "store_df = pd.read_csv(file_path[2], low_memory=False, dtype=str)\n",
    "store_states_df = pd.read_csv(file_path[3], low_memory=False,  dtype=str)\n",
    "\n",
    "print(f\"Size of the store df: {store_df.shape}\")\n",
    "\n",
    "train_df = pd.read_csv(file_path[0], low_memory=False, dtype=str) # split this into test train for validation ++ we may also need to reverse the order of the data\n",
    "test_df = pd.read_csv(file_path[1], low_memory=False, dtype=str)\n",
    "train_df = pd.merge(train_df, store_df, how=\"inner\", on=\"Store\") #  one thing to note is that when they pre process data, they keep the store and test data separate, \n",
    "test_df = pd.merge(test_df, store_df, how=\"inner\", on=\"Store\")\n",
    "train_df['Date'] = pd.to_datetime(train_df['Date'])\n",
    "train_df['Year'], train_df['Month'], train_df['Day'] = train_df['Date'].dt.year.values, train_df['Date'].dt.month.values, train_df['Date'].dt.day.values\n",
    "train_df.drop(['Date'], axis=1, inplace=True)\n",
    "train_df = pd.merge(train_df, store_states_df, how=\"inner\", on=\"Store\")\n",
    "test_df = pd.merge(test_df, store_states_df, how=\"inner\", on=\"Store\")\n",
    "\n",
    "print(f\"The column names before dropping are : {train_df.columns.tolist()}\")\n",
    "print(f\"Size of the train dataset after merging is : {train_df.shape}\")\n",
    "# print(f\"Size of the test dataset after merging is : {test_df.shape}\")\n",
    "print(train_df['Store'].dtype)\n",
    "\n",
    "# helper function to replace nan values if any\n",
    "train_df.fillna('0', inplace=True)\n",
    "\n",
    "train_df = train_df[train_df[\"Sales\"]!=\"0\"]\n",
    "train_df = train_df[train_df[\"Open\"]!=\"\"]\n",
    "cols_to_drop = ['StateHoliday', 'SchoolHoliday', 'CompetitionOpenSinceMonth', 'StoreType', 'Assortment', 'PromoInterval', 'Promo2SinceWeek', 'Promo2SinceYear', 'Promo2', 'CompetitionOpenSinceYear', 'CompetitionDistance', 'Customers']\n",
    "\n",
    "train_df.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "train_data_y = pd.DataFrame(train_df['Sales'])\n",
    "train_data_x = train_df.drop(['Sales'], axis=1)\n",
    "\n",
    "for feature in train_data_x.columns: # this is to convert the categorical data into numerical data\n",
    "    label_encoder = preprocessing.LabelEncoder()\n",
    "    train_data_x.loc[:,feature] = label_encoder.fit_transform(train_data_x[feature].astype(str).fillna(\"0\").values)\n",
    "train_data_x = train_data_x.reindex(['Open','Store', 'DayOfWeek',  'Promo', 'Year', 'Month', 'Day' ,'State'], axis=1)\n",
    "# train_data_x = train_data_x.astype(int)\n",
    "# train_data_y = train_data_y.astype(int)\n",
    "print(f\"Size of the train dataset after merging and preprocessing is : {train_df.shape}\")\n",
    "##############################################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>Store</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>Promo</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>State</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>24</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>23</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Open Store DayOfWeek Promo  Year  Month  Day State\n",
       "0     0     0         4     1     2      9   24     4\n",
       "1     0     0         3     1     2      9   23     4\n",
       "2     0     0         2     1     2      9   21     4\n",
       "3     0     0         1     1     2      9   20     4\n",
       "4     0     0         0     1     2      9   19     4\n",
       "6     0     0         5     0     2      9   17     4\n",
       "7     0     0         4     0     2      9   16     4\n",
       "8     0     0         3     0     2      9   15     4\n",
       "9     0     0         2     0     2      9   14     4\n",
       "10    0     0         1     0     2      9   13     4"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_x.head(10) # shows random 10 instead of first ten like head\n",
    "# print(train_data_x.shape)\n",
    "\n",
    "# they kept 8 columns as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(844338, 8)\n",
      "(844338, 1)\n"
     ]
    }
   ],
   "source": [
    "print(train_data_x.shape) # matches\n",
    "print(train_data_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import pickle\n",
    "\n",
    "train_ratio = 0.9\n",
    "shuffle_data = False\n",
    "one_hot_as_input = False\n",
    "embeddings_as_input = False\n",
    "save_embeddings = True\n",
    "saved_embeddings_fname = \"embeddings.pickle\"  # set save_embeddings to True to create this file\n",
    "\n",
    "train_data_x = train_data_x.astype(np.int64)\n",
    "train_data_y = train_data_y.astype(np.int64)\n",
    "\n",
    "def split_features(X): # this function takes a numpy array and splits it into a list of np arrays. It returns a list of all the features/columns in the dataset\n",
    " # Extract the column with index 0 (store) and append to the list\n",
    "    X_list = []\n",
    "    store_index = X[:, 0].reshape(-1, 1)\n",
    "    X_list.append(np.array(store_index, dtype=np.int64))\n",
    "\n",
    "    # Extract the column with index 1 (day_of_week) and append to the list\n",
    "    day_of_week = X[:, 1].reshape(-1, 1)\n",
    "    X_list.append(np.array(day_of_week, dtype=np.int64))\n",
    "\n",
    "    # Extract the column with index 2 (promo) and append to the list\n",
    "    promo = X[:, 2].reshape(-1, 1)\n",
    "    X_list.append(np.array(promo, dtype=np.int64))\n",
    "\n",
    "    # Extract the column with index 3 (year) and append to the list\n",
    "    year = X[:, 3].reshape(-1, 1)\n",
    "    X_list.append(np.array(year, dtype=np.int64))\n",
    "\n",
    "    # Extract the column with index 4 (month) and append to the list\n",
    "    month = X[:, 4].reshape(-1, 1)\n",
    "    X_list.append(np.array(month, dtype=np.int64))\n",
    "\n",
    "    # Extract the column with index 5 (day) and append to the list\n",
    "    day = X[:, 5].reshape(-1, 1)\n",
    "    X_list.append(np.array(day, dtype=np.int64))\n",
    "    # Extract the column with index 6 (state) and append to the list\n",
    "    state = X[:, 6].reshape(-1, 1)\n",
    "    X_list.append(np.array(state, dtype=np.int64))\n",
    "\n",
    "    return X_list\n",
    " \n",
    "split_features(train_data_x.values)\n",
    "\n",
    "\n",
    "# okay they ar enot really suing this/ they only use this to feed embeddings leanred duing NN to other models as input\n",
    "def embed_features(X, saved_embeddings_fname): # this function creates and saves the embeddings for the categorical data\n",
    "   f_embeddings = open(saved_embeddings_fname, \"rb\")\n",
    "   embeddings = pickle.load(f_embeddings)\n",
    "\n",
    "   index_embedding_mapping = {1: 0, 2: 1, 4: 2, 5: 3, 6: 4, 7: 5} # this is the mapping of the columns index to the embeddings index. They skipped columns 1 and 3. Opena and Promo because they are 0, 1? I guess binary categories are useless to embed. Embedding these binary features may not provide significant benefits, as their information is already somewhat captured by the binary nature.\n",
    "   X_embedded = []\n",
    "   print(X.shape)\n",
    "\n",
    "   (num_records, num_features) = X.shape\n",
    "   for record in X: # a record is a row in the dataset\n",
    "      # print(record)\n",
    "      embedded_features = []\n",
    "      for i, feat in enumerate(record): # this is to embed the features\n",
    "         # print(i, feat)\n",
    "         feat = int(feat)\n",
    "         if i not in index_embedding_mapping.keys():\n",
    "               embedded_features += [feat]\n",
    "         else:\n",
    "               embedding_index = index_embedding_mapping[i]\n",
    "               embedded_features += embeddings[embedding_index][feat].tolist()\n",
    "\n",
    "      X_embedded.append(embedded_features)\n",
    "\n",
    "   return pd.DataFrame(X_embedded) # returns the embedded features\n",
    "   # return np.array(X_embedded) # returns the embedded features\n",
    "\n",
    "# j = embed_features((np.array(train_data_x)), saved_embeddings_fname)\n",
    "\n",
    "# print(j)\n",
    "\n",
    "# print(j.shape) # okay shape matches theirs as well\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Sales\n",
      "934085   6760\n",
      "937882   6833\n",
      "977052   8578\n",
      "165948   8467\n",
      "658089   5214\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# train_data_x = j\n",
    "# train_data_y = np.array(train_data_y)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_data_x, train_data_y, test_size=0.1, random_state=42) # this is to split the data into train and test sets\n",
    "print(y_train[:5])\n",
    "for data in [X_train, X_val]:\n",
    "    data.drop(['Open'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So basically there is an entity embedding layer for each categoircal feature (Store, DOW, etc). OPen and Promo are not inlcuded since they are binary. \n",
    "These are the embedding dimesions for different categories used in the paper            \n",
    "cat_emb_dim={\n",
    "                'Store': 10,\n",
    "                'DayOfWeek': 6,\n",
    "                'Promo': 1,\n",
    "                'Year': 2,\n",
    "                'Month': 6,\n",
    "                'Day': 10,\n",
    "                'State': 6},}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Sales\n",
      "934085   6760\n",
      "937882   6833\n",
      "977052   8578\n",
      "165948   8467\n",
      "658089   5214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/loss.py:101: UserWarning: Using a target size (torch.Size([128, 1])) that is different to the input size (torch.Size([128])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/loss.py:101: UserWarning: Using a target size (torch.Size([96, 1])) that is different to the input size (torch.Size([96])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.030744114890694618\n",
      "Epoch 2/10, Loss: 0.030887659639120102\n",
      "Epoch 3/10, Loss: 0.03071647882461548\n",
      "Epoch 4/10, Loss: 0.030838701874017715\n",
      "Epoch 5/10, Loss: 0.03091205097734928\n",
      "Epoch 6/10, Loss: 0.030927734449505806\n",
      "Epoch 7/10, Loss: 0.03092590905725956\n",
      "Epoch 8/10, Loss: 0.030922962352633476\n",
      "Epoch 9/10, Loss: 0.030890056863427162\n",
      "Epoch 10/10, Loss: 0.03087809681892395\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/aisha/Desktop/ML Proj/Proj_ML.ipynb Cell 10\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aisha/Desktop/ML%20Proj/Proj_ML.ipynb#X33sZmlsZQ%3D%3D?line=91'>92</a>\u001b[0m \u001b[39m# Prediction example\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aisha/Desktop/ML%20Proj/Proj_ML.ipynb#X33sZmlsZQ%3D%3D?line=92'>93</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/aisha/Desktop/ML%20Proj/Proj_ML.ipynb#X33sZmlsZQ%3D%3D?line=93'>94</a>\u001b[0m     output \u001b[39m=\u001b[39m pytorch_model(\u001b[39m*\u001b[39m[torch\u001b[39m.\u001b[39mfrom_numpy(example)\u001b[39m.\u001b[39mlong() \u001b[39mfor\u001b[39;00m example \u001b[39min\u001b[39;00m features])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aisha/Desktop/ML%20Proj/Proj_ML.ipynb#X33sZmlsZQ%3D%3D?line=94'>95</a>\u001b[0m     prediction \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aisha/Desktop/ML%20Proj/Proj_ML.ipynb#X33sZmlsZQ%3D%3D?line=95'>96</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mPrediction:\u001b[39m\u001b[39m\"\u001b[39m, prediction)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'features' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PyTorchModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PyTorchModel, self).__init__()\n",
    "        \n",
    "        # Define Embeddings\n",
    "        self.store_embedding = nn.Embedding(1115, 10)\n",
    "        self.dow_embedding = nn.Embedding(7, 6)\n",
    "        self.year_embedding = nn.Embedding(3, 2)\n",
    "        self.month_embedding = nn.Embedding(12, 6)\n",
    "        self.day_embedding = nn.Embedding(31, 10)\n",
    "        self.state_embedding = nn.Embedding(12, 6)\n",
    "        \n",
    "        # Define other layers\n",
    "        self.promo_layer = nn.Linear(1, 1)\n",
    "        self.dense1 = nn.Linear(41, 1000)\n",
    "        self.dense2 = nn.Linear(1000, 500)\n",
    "        self.dense3 = nn.Linear(500, 1)\n",
    "        \n",
    "    def forward(self, input_store, input_dow, input_promo, input_year, input_month, input_day, input_state):\n",
    "        # Apply embeddings\n",
    "        input_promo = input_promo.unsqueeze(1)\n",
    "        output_store = self.store_embedding(input_store.view(-1, 1))\n",
    "        output_dow = self.dow_embedding(input_dow.view(-1, 1))\n",
    "        output_year = self.year_embedding(input_year.view(-1, 1))\n",
    "        output_month = self.month_embedding(input_month.view(-1, 1))\n",
    "        output_day = self.day_embedding(input_day.view(-1, 1))\n",
    "        output_state = self.state_embedding(input_state.view(-1, 1))\n",
    "        \n",
    "        # Reshape embeddings\n",
    "        output_store = output_store.view(-1, 10)\n",
    "        output_dow = output_dow.view(-1, 6)\n",
    "        output_year = output_year.view(-1, 2)\n",
    "        output_month = output_month.view(-1, 6)\n",
    "        output_day = output_day.view(-1, 10)\n",
    "        output_state = output_state.view(-1, 6)\n",
    "        \n",
    "        # print(output_store.shape)\n",
    "        # print(output_dow.shape)\n",
    "        # print(input_promo.shape)\n",
    "                \n",
    "        # Concatenate embeddings\n",
    "        concatenated = torch.cat([output_store, output_dow, input_promo, output_year, output_month, output_day, output_state], dim=1)\n",
    "        # print(concatenated.shape)\n",
    "        # Feed forward through other layers\n",
    "        x = F.relu(self.dense1(concatenated))\n",
    "        x = F.relu(self.dense2(x))\n",
    "        x = self.dense3(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "pytorch_model = PyTorchModel()\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = optim.Adam(pytorch_model.parameters(), lr=0.001)\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "print(y_train[:5])\n",
    "max_log_y = max(np.max(np.log(y_train)), np.max(np.log(y_val))) # this is to normalize the data\n",
    "y_train = np.log(y_train)/max_log_y\n",
    "# print(\"After \\n\",y_train[:5])\n",
    "y_val = np.log(y_val)/max_log_y\n",
    "X_train_tensor = torch.from_numpy(np.array(X_train))\n",
    "y_train_tensor = torch.from_numpy(np.array(y_train))\n",
    "\n",
    "# Training loop\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        batch_X = X_train_tensor[i:i+batch_size]\n",
    "        batch_y = y_train_tensor[i:i+batch_size]\n",
    "        # print(batch_X.shape)\n",
    "        # print(batch_y.shape)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = pytorch_model(batch_X[:, 0], batch_X[:, 1], batch_X[:, 2], batch_X[:, 3], batch_X[:, 4], batch_X[:, 5], batch_X[:, 6])\n",
    "\n",
    "        loss = criterion(output.flatten(), batch_y.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss.item()}')\n",
    "\n",
    "# # Prediction example\n",
    "# with torch.no_grad():\n",
    "#     output = pytorch_model(*[torch.from_numpy(example).long() for example in features])\n",
    "#     prediction = output.numpy()\n",
    "#     print(\"Prediction:\", prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Fri Jun  1 17:51:34 2018\n",
    "\n",
    "@author: raulsanchez\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.utils.validation import check_X_y\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data_utils\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class NeuralNet(nn.Module, BaseEstimator, RegressorMixin):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    cat_emb_dim : dict\n",
    "        Dictionary containing the embedding sizes.\n",
    "\n",
    "    layers : list\n",
    "        NN. Layer arquitecture\n",
    "    drop_out_layers : dict\n",
    "        Dictionary with layer dropout\n",
    "    drop_out_emb : float\n",
    "        embedding drop out\n",
    "    batch_size : int\n",
    "        Mini Batch size\n",
    "    val_idx : list\n",
    "\n",
    "    allow_cuda : bool\n",
    "\n",
    "    act_func : string\n",
    "\n",
    "    lr : float\n",
    "\n",
    "    alpha : float\n",
    "\n",
    "    epochs : int\n",
    "    '''\n",
    "    def __init__(\n",
    "        self,\n",
    "        act_func='relu',\n",
    "        train_size=.8,\n",
    "        batch_size=128,\n",
    "        random_seed=None,\n",
    "        verbose=True,\n",
    "        verbose_epoch=100):\n",
    "\n",
    "        super(NeuralNet, self).__init__()\n",
    "\n",
    "        # General\n",
    "        self.act_func = act_func\n",
    "        self.train_size = train_size\n",
    "        self.batch_size = int(batch_size)\n",
    "        self.verbose = verbose\n",
    "        self.verbose_epoch = verbose_epoch\n",
    "        self.random_seed = random_seed\n",
    "\n",
    "        if not(self.random_seed is None):\n",
    "            torch.manual_seed(self.random_seed)\n",
    "\n",
    "    def activ_func(self, x):\n",
    "        '''\n",
    "        Applies an activation function\n",
    "        '''\n",
    "\n",
    "        act_funcs = {\n",
    "            'relu': F.relu,\n",
    "            'selu': F.selu}\n",
    "\n",
    "        return act_funcs[self.act_func](x)\n",
    "\n",
    "    def make_dataloader(self, X, y=None, shuffle=False, num_workers=8):\n",
    "        '''\n",
    "        Wraps a dataloader to iterate over (X, y)\n",
    "        '''\n",
    "        if y is not None and isinstance(y, pd.DataFrame):\n",
    "            y = y.values.ravel()\n",
    "\n",
    "        check_X_y(X, y)\n",
    "\n",
    "        loader = data_utils.DataLoader(\n",
    "            data_utils.TensorDataset(\n",
    "                torch.from_numpy(X.values).float(),\n",
    "                torch.from_numpy(y).float() if y is not None else None\n",
    "            ),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=shuffle,\n",
    "            num_workers=num_workers)\n",
    "\n",
    "        return loader\n",
    "\n",
    "    def split_train_test(self):\n",
    "        '''\n",
    "        Splits Train-Test partitions\n",
    "        '''\n",
    "\n",
    "        err_msg = 'X size %s does not match y size %s'\n",
    "        assert self.X.shape[0] == self.y.shape[0], err_msg % (\n",
    "            self.X.shape, self.y.shape)\n",
    "\n",
    "        if (self.train_size < 1) and (self.train_size > 0):\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                self.X, self.y, train_size=self.train_size)\n",
    "        else:\n",
    "            X_train = self.X\n",
    "            X_test = self.X\n",
    "            y_train = self.y\n",
    "            y_test = self.y\n",
    "\n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "\n",
    "\n",
    "class EntEmbNN(NeuralNet):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    cat_emb_dim : dict\n",
    "        Dictionary containing the embedding sizes.\n",
    "\n",
    "    layers : list\n",
    "        NN. Layer arquitecture\n",
    "    drop_out_layers : dict\n",
    "        Dictionary with layer dropout\n",
    "    drop_out_emb : float\n",
    "        embedding drop out\n",
    "    batch_size : int\n",
    "        Mini Batch size\n",
    "    val_idx : list\n",
    "\n",
    "    allow_cuda : bool\n",
    "\n",
    "    act_func : string\n",
    "\n",
    "    lr : float\n",
    "\n",
    "    alpha : float\n",
    "\n",
    "    epochs : int\n",
    "    '''\n",
    "    def __init__(\n",
    "        self,\n",
    "        cat_emb_dim = {},\n",
    "        dense_layers = [1000, 500],\n",
    "        drop_out_layers = [0., 0.],\n",
    "        drop_out_emb = 0.,\n",
    "        act_func = 'relu',\n",
    "        loss_function='MSELoss',\n",
    "        train_size=1.,\n",
    "        batch_size=128,\n",
    "        epochs=10,\n",
    "        lr=0.001,\n",
    "        alpha=0.0,\n",
    "        rand_seed=1,\n",
    "        allow_cuda=False,\n",
    "        random_seed=None,\n",
    "        output_sigmoid=False,\n",
    "        verbose=True):\n",
    "\n",
    "        super(EntEmbNN, self).__init__()\n",
    "\n",
    "        # Model specific params.\n",
    "        self.cat_emb_dim = cat_emb_dim\n",
    "        self.dense_layers = dense_layers\n",
    "        self.output_sigmoid = output_sigmoid\n",
    "\n",
    "        # Reg. parameters\n",
    "        self.drop_out_layers = drop_out_layers\n",
    "        self.drop_out_emb = drop_out_emb\n",
    "        self.alpha = alpha\n",
    "\n",
    "        # Training params\n",
    "        self.act_func = act_func\n",
    "        self.train_size = train_size\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.loss_function = loss_function\n",
    "\n",
    "        # Misc\n",
    "        self.allow_cuda = allow_cuda\n",
    "        self.verbose = verbose\n",
    "        self.random_seed = random_seed\n",
    "\n",
    "        # Internal\n",
    "        self.embeddings = {}\n",
    "        self.train_loss = []\n",
    "        self.train_epoch_loss = []\n",
    "        self.epochs_reports = []\n",
    "\n",
    "        self.labelencoders = {}\n",
    "        self.scaler = None\n",
    "\n",
    "        self.num_features = None\n",
    "        self.cat_features = None\n",
    "        self.layers = {}\n",
    "\n",
    "    def get_loss(self, loss_name):\n",
    "        if loss_name == 'SmoothL1Loss':\n",
    "            return torch.nn.SmoothL1Loss()\n",
    "        elif loss_name == 'L1Loss':\n",
    "            return torch.nn.L1Loss()\n",
    "        elif loss_name == 'MSELoss':\n",
    "            return torch.nn.MSELoss()\n",
    "        elif loss_name == 'BCELoss':\n",
    "            return torch.nn.BCELoss()\n",
    "        elif loss_name == 'BCEWithLogitsLoss':\n",
    "            return torch.nn.BCEWithLogitsLoss()\n",
    "        else:\n",
    "            print(\n",
    "                'Invalid Loss name: %s, using default: %s' % (\n",
    "                    loss_name, 'MSELoss')\n",
    "            )\n",
    "            return torch.nn.MSELoss()\n",
    "\n",
    "    def init_embeddings(self):\n",
    "        '''\n",
    "        Initializes the embeddings\n",
    "        '''\n",
    "\n",
    "        # Get embedding sizes from categ. features\n",
    "        for f in self.cat_features:\n",
    "            le = self.labelencoders[f]\n",
    "\n",
    "            emb_dim = self.cat_emb_dim[f]\n",
    "\n",
    "            self.embeddings[f] = nn.Embedding(\n",
    "                len(le.classes_),\n",
    "                emb_dim)\n",
    "\n",
    "            # Weight initialization as original paper\n",
    "            torch.nn.init.uniform_(\n",
    "                self.embeddings[f].weight.data,\n",
    "                a=-.05, b=.05)\n",
    "\n",
    "            # Add emb. layer to model\n",
    "            self.add_module(\n",
    "                '[Emb %s]' % f,\n",
    "                self.embeddings[f])\n",
    "\n",
    "    def init_dense_layers(self):\n",
    "        '''\n",
    "        Initializes dense layers\n",
    "        '''\n",
    "\n",
    "        input_size = (\n",
    "            # Numb of Embedding neurons in input layer\n",
    "            sum([\n",
    "                self.embeddings[f].weight.data.shape[1]\n",
    "                for f in self.cat_features\n",
    "            ])\n",
    "        ) + (\n",
    "            # Numb of regular neurons for numerical features\n",
    "            len(self.num_features)\n",
    "        )\n",
    "\n",
    "        NN_arquitecture = (\n",
    "            [input_size]\n",
    "        ) + (\n",
    "            self.dense_layers\n",
    "        ) + (\n",
    "            [1]\n",
    "        )\n",
    "\n",
    "        for layer_idx, current_layer_size in enumerate(NN_arquitecture[:-1]):\n",
    "            next_layer_size = NN_arquitecture[layer_idx + 1]\n",
    "\n",
    "            layer_name = 'l%s' % (layer_idx + 1)\n",
    "            layer = nn.Linear(current_layer_size, next_layer_size)\n",
    "\n",
    "            self.add_module(layer_name, layer)\n",
    "\n",
    "            self.layers[layer_name] = layer\n",
    "\n",
    "    def X_fit(self, X):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        # Identify categorical vs numerical features\n",
    "        self.cat_features = list(self.cat_emb_dim.keys())\n",
    "        self.num_features = list(set(\n",
    "            self.X.columns.tolist()\n",
    "        ).difference(self.cat_features))\n",
    "\n",
    "        # Create encoders for categorical features\n",
    "        self.labelencoders = {}\n",
    "        for c in self.cat_features:\n",
    "            le = LabelEncoder()\n",
    "            le.fit( X[c].astype(str).tolist())\n",
    "            self.labelencoders[c] = le\n",
    "\n",
    "    def X_transform(self, X):\n",
    "        \"\"\"\n",
    "        X = X_train.iloc[test_idx]\n",
    "        \"\"\"\n",
    "        X = X.copy()\n",
    "        for c in self.cat_features:\n",
    "            codes = X[c].astype(str)\n",
    "\n",
    "            missin_codes = ~codes.isin(self.labelencoders[c].classes_)\n",
    "\n",
    "            codes[missin_codes] = self.labelencoders[c].classes_[0]\n",
    "\n",
    "            X[c] = self.labelencoders[c].transform(\n",
    "                codes\n",
    "            )\n",
    "\n",
    "        X = X[self.cat_features + self.num_features]\n",
    "\n",
    "        return X\n",
    "\n",
    "    def X_emd_replace(self, data):\n",
    "        '''\n",
    "        Returns the formated X-input, which is composed by the categorical\n",
    "        embeddings and the respective continuous inputs.\n",
    "        '''\n",
    "\n",
    "        ''' Replace embeddings '''\n",
    "        data_emb = []\n",
    "        for f_idx, f in enumerate(self.cat_features):\n",
    "            # Get column feature\n",
    "            f_data = data[:, f_idx]\n",
    "\n",
    "            if self.allow_cuda:\n",
    "                f_data = f_data.cuda()\n",
    "\n",
    "            # Retrieves the embeddings\n",
    "            emb_cat = self.embeddings[f](f_data.long())\n",
    "\n",
    "            #Apply Dropout\n",
    "            emb_cat = F.dropout(\n",
    "                emb_cat,\n",
    "                p=self.drop_out_emb,\n",
    "                training=self.training)\n",
    "\n",
    "            data_emb.append(emb_cat)\n",
    "\n",
    "        ''' Concat numeric features '''\n",
    "        if len(self.num_features) > 0:\n",
    "            data_emb.append(\n",
    "                data[:, len(self.cat_features):]\n",
    "            )\n",
    "\n",
    "        return torch.cat(data_emb, 1)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.X = X.copy()\n",
    "        self.y = y.copy()\n",
    "\n",
    "        self.X_fit(self.X)\n",
    "\n",
    "        self.split_train_test()\n",
    "\n",
    "        # Create embeddings and layers\n",
    "        self.init_embeddings()\n",
    "        self.init_dense_layers()\n",
    "\n",
    "        # GPU Flag\n",
    "        if self.allow_cuda:\n",
    "            self.cuda()\n",
    "\n",
    "        self.iterate_n_epochs(epochs=self.epochs)\n",
    "\n",
    "    def iterate_n_epochs(self, epochs):\n",
    "        '''\n",
    "        Makes N training iterations\n",
    "        epochs = self.epochs\n",
    "        '''\n",
    "\n",
    "        self.epoch_cnt = 0\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            self.parameters(),\n",
    "            lr=self.lr,\n",
    "            weight_decay=self.alpha\n",
    "        )\n",
    "\n",
    "        while(self.epoch_cnt < epochs):\n",
    "            self.train()\n",
    "            loss_func = self.get_loss(self.loss_function)\n",
    "\n",
    "            dataloader = self.make_dataloader(\n",
    "                # Format X such as categ.first then numeric.\n",
    "                self.X_transform(self.X_train),\n",
    "                self.y_train,\n",
    "                shuffle=True,\n",
    "            )\n",
    "\n",
    "            train_epoch_loss = []\n",
    "            for batch_idx, (x, target) in enumerate(dataloader):\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                if self.allow_cuda:\n",
    "                    x, target = x.cuda(), target.cuda()\n",
    "                x, target = Variable(x), Variable(target).float()\n",
    "\n",
    "                self.X_train.iloc[0]\n",
    "                output = self.forward(x)\n",
    "\n",
    "                loss = loss_func(\n",
    "                    output.reshape(1, -1)[0],\n",
    "                    target.float())\n",
    "\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                train_epoch_loss.append(loss.item())\n",
    "\n",
    "                # if (batch_idx % self.verbose_epoch) == 0:\n",
    "                #     if self.verbose:\n",
    "                #         print('\\t\\t%s' % (\n",
    "                #             sum(train_epoch_loss) / len(train_epoch_loss)\n",
    "                #             )\n",
    "                #         )\n",
    "\n",
    "            self.train_epoch_loss.append(\n",
    "                sum(train_epoch_loss) / len(train_epoch_loss)\n",
    "            )\n",
    "            self.train_loss += train_epoch_loss\n",
    "\n",
    "            self.epoch_cnt += 1\n",
    "            # self.eval_model()\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Forward pass\n",
    "        x_ = x\n",
    "        '''\n",
    "\n",
    "        # Parse batch with embeddings\n",
    "        x = self.X_emd_replace(x)\n",
    "\n",
    "        # Forward pass on dense layers\n",
    "        n_layers = len(self.layers.items())\n",
    "        for layer_idx in range(n_layers):\n",
    "            layer_name = 'l%s' % (layer_idx + 1)\n",
    "            layer = self.layers[layer_name]\n",
    "\n",
    "            is_inner_layer = (\n",
    "                layer_idx < len(self.dense_layers)\n",
    "            )\n",
    "\n",
    "            x = layer(x)\n",
    "\n",
    "            # Do not apply act.func on last layer\n",
    "            if is_inner_layer:\n",
    "\n",
    "                # Apply dropout\n",
    "                x = F.dropout(\n",
    "                    x,\n",
    "                    p=self.drop_out_layers[layer_idx],\n",
    "                    training=self.training)\n",
    "\n",
    "                x = self.activ_func(x)\n",
    "\n",
    "            elif self.output_sigmoid:\n",
    "                x = torch.sigmoid(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def predict_raw(self, X):\n",
    "        '''\n",
    "        Predict scores\n",
    "\n",
    "        self = NNmodel\n",
    "        X  = self.X_test\n",
    "        '''\n",
    "\n",
    "        #Set pytorch model in eval. mode\n",
    "        self.eval()\n",
    "\n",
    "        dataloader = self.make_dataloader(self.X_transform(X))\n",
    "\n",
    "        y_pred = []\n",
    "        for batch_idx, (x, _) in enumerate(dataloader):\n",
    "            if self.allow_cuda:\n",
    "                x = x.cuda()\n",
    "            x = Variable(x)\n",
    "\n",
    "            output = self.forward(x)\n",
    "\n",
    "            if self.allow_cuda:\n",
    "                output = output.cpu()\n",
    "            y_pred += output.data.numpy().flatten().tolist()\n",
    "\n",
    "        y_pred = np.array(y_pred)\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "#    def get_embeddings(self):\n",
    "#\n",
    "#        embeddings = {}\n",
    "#        for c in self.cat_features:\n",
    "#            categ_names = self.X[c].drop_duplicates()\n",
    "#            categ_codes = categ_names.cat.codes\n",
    "#            categories = pd.Series(\n",
    "#                [x for x in categ_names],\n",
    "#                index=categ_codes.values)\n",
    "#            categories.sort_index(inplace=True)\n",
    "#            categories.index = categories.index + 1\n",
    "#\n",
    "#            emb = self.embeddings[c].weight.data\n",
    "#            if self.allow_cuda:\n",
    "#                emb = emb.cpu()\n",
    "#\n",
    "#            emb = pd.DataFrame(\n",
    "#                emb.numpy(),\n",
    "#                index=categories.values)\n",
    "#            emb = emb.add_prefix('latent_')\n",
    "#            embeddings[c] = emb\n",
    "#\n",
    "#        return embeddings\n",
    "    def dump_embeddings(self, emb_path):\n",
    "        \"\"\"\n",
    "        Dump embeddings to hdf file.\n",
    "        \"\"\"\n",
    "        print('Saving in: %s' % emb_path)\n",
    "        for emb_name, emb_pytorch in self.embeddings.items():\n",
    "            emb = pd.DataFrame(\n",
    "                emb_pytorch.weight.data.numpy(),\n",
    "                index=self.labelencoders[emb_name].classes_)\n",
    "            print('\\t%s' % emb_name)\n",
    "\n",
    "            emb.to_hdf(emb_path, key=emb_name)\n",
    "\n",
    "    def get_data_embeddings(self, X_raw):\n",
    "        \"\"\"\n",
    "        Transforma a X matrix substituing the embeddings on the X matrix.\n",
    "        \"\"\"\n",
    "\n",
    "        data_emb = []\n",
    "        for emb_name in self.cat_features:\n",
    "\n",
    "            emb_pytorch = self.embeddings[emb_name]\n",
    "\n",
    "            emb = pd.DataFrame(\n",
    "                emb_pytorch.weight.data.numpy(),\n",
    "                index=self.labelencoders[emb_name].classes_\n",
    "            ).add_prefix('%s_' % emb_name)\n",
    "\n",
    "            x = emb.loc[X_raw[emb_name]]\n",
    "            x = x.reset_index().drop('index', axis=1)\n",
    "            x.index = X_raw.index\n",
    "\n",
    "            data_emb.append(x)\n",
    "\n",
    "        data_emb.append(X_raw[self.num_features])\n",
    "\n",
    "        X_emb = pd.concat(data_emb, axis=1)\n",
    "\n",
    "        return X_emb\n",
    "\n",
    "    def load(self, filename):\n",
    "        \"\"\"\n",
    "        Loads a saved model\n",
    "        \"\"\"\n",
    "\n",
    "        f = open(filename, 'rb')\n",
    "        tmp_dict = pickle.load(f)\n",
    "        f.close()\n",
    "\n",
    "        self.__dict__.update(tmp_dict)\n",
    "\n",
    "    def save(self, filename):\n",
    "        \"\"\"\n",
    "        Saves as pickle object\n",
    "        \"\"\"\n",
    "\n",
    "        f = open(filename, 'wb')\n",
    "        pickle.dump(self.__dict__, f, 2)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Jun 26 21:54:50 2018\n",
    "\n",
    "@author: raulsanchez\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "def RMSLE(y_true, y_pred): \n",
    "    \"\"\"\n",
    "    Root Mean Squared Logarithmic Error\n",
    "    \"\"\"\n",
    "    n = y_true.shape[0]\n",
    "    \n",
    "    return np.sqrt(\n",
    "        (1 /n)  * (\n",
    "            ( np.log1p(y_pred) - np.log1p(y_true) ) ** 2\n",
    "        ).sum()\n",
    "        )\n",
    "    \n",
    "    \n",
    "def gini(solution, submission):\n",
    "    '''\n",
    "    '''                                 \n",
    "    \n",
    "    df = sorted(zip(solution, submission), key=lambda x : (x[1], x[0]),  reverse=True)\n",
    "    random = [float(i+1)/float(len(df)) for i in range(len(df))]                \n",
    "    totalPos = np.sum([x[0] for x in df])                                       \n",
    "    cumPosFound = np.cumsum([x[0] for x in df])                                     \n",
    "    Lorentz = [float(x)/totalPos for x in cumPosFound]                          \n",
    "    Gini = [l - r for l, r in zip(Lorentz, random)]                             \n",
    "    return np.sum(Gini)                                                         \n",
    "\n",
    "\n",
    "def gini_norm(y_pred, y_true):\n",
    "    '''\n",
    "    '''\n",
    "    \n",
    "    normalized_gini = gini(y_pred, y_true)/gini(y_true, y_true)\n",
    "    return normalized_gini    \n",
    "\n",
    "def MAPE(y_true, y_pred):\n",
    "    '''\n",
    "    mean absolute percentage error\n",
    "    '''\n",
    "    \n",
    "    abs_err = np.absolute((y_true - y_pred))\n",
    "    percent_err = abs_err / y_true\n",
    "    mape = np.sum(percent_err) / len(y_true)\n",
    "    \n",
    "    return mape\n",
    "\n",
    "def RMSPE(y_true, y_pred):\n",
    "    '''\n",
    "    Root Mean Square Percentage Error (RMSPE).\n",
    "    '''\n",
    "    square_percent_err = ((y_true - y_pred) / y_true) ** 2\n",
    "    mean_square_percent_err = pd.Series(square_percent_err).mean()\n",
    "    \n",
    "    rmspe = np.sqrt(mean_square_percent_err)\n",
    "    \n",
    "    return rmspe\n",
    "\n",
    "def eval_regression(y_true, y_pred):\n",
    "    reg_metrics = {\n",
    "        'mean_absolute_error': metrics.mean_absolute_error(\n",
    "            y_true=y_true, \n",
    "            y_pred=y_pred),\n",
    "        'explained_variance_score': metrics.explained_variance_score(\n",
    "            y_true=y_true, \n",
    "            y_pred=y_pred),\n",
    "        'mean_squared_error': metrics.mean_squared_error(\n",
    "            y_true=y_true, \n",
    "            y_pred=y_pred),\n",
    "        'median_absolute_error': metrics.median_absolute_error(\n",
    "            y_true=y_true, \n",
    "            y_pred=y_pred),\n",
    "        'r2_score': metrics.r2_score(\n",
    "            y_true=y_true, \n",
    "            y_pred=y_pred),\n",
    "        'gini_normalized': gini_norm(\n",
    "            y_true=y_true, \n",
    "            y_pred=y_pred),\n",
    "        'mean_absolute_percentage_error': MAPE(\n",
    "            y_true=y_true, \n",
    "            y_pred=y_pred),\n",
    "        'root_mean_squared_logarithmic_error': RMSLE(\n",
    "            y_true=y_true, \n",
    "            y_pred=y_pred)\n",
    "        }\n",
    "        \n",
    "    return pd.Series(reg_metrics)\n",
    "\n",
    "    \n",
    "def classification_report(y_true, y_pred, y_score=None, average='micro'):\n",
    "    '''\n",
    "    Params:\n",
    "    --------\n",
    "    y_true : 1d array-like, or label indicator array / sparse matrix\n",
    "    Ground truth (correct) target values.\n",
    "\n",
    "    y_pred : 1d array-like, or label indicator array / sparse matrix\n",
    "    Estimated targets as returned by a classifier.\n",
    "    \n",
    "    y_score : nd array-like with the probabilities of the classes.\n",
    "    \n",
    "    average : str. either 'micro' or 'macro', for more details\n",
    "        of how they are computed see:\n",
    "            http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html#multiclass-settings\n",
    "    Return:\n",
    "    --------\n",
    "    pd.DataFrame : contains the classification report as pandas.DataFrame \n",
    "    \n",
    "    Example:\n",
    "    ---------\n",
    "    from sklearn.metrics import classification_report\n",
    "    \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.datasets import make_classification\n",
    "    \n",
    "    X, y = make_classification(n_samples=5000, n_features=10,\n",
    "                               n_informative=5, n_redundant=0,\n",
    "                               n_classes=10, random_state=0, \n",
    "                               shuffle=False)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    \n",
    "    model = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    sk_report = classification_report(\n",
    "        digits=6,\n",
    "        y_true=y_test, \n",
    "        y_pred=model.predict(X_test))\n",
    "    \n",
    "    report_with_auc = class_report(\n",
    "        y_true=y_test, \n",
    "        y_pred=model.predict(X_test), \n",
    "        y_score=model.predict_proba(X_test))\n",
    "    \n",
    "    print(sk_report)\n",
    "    \n",
    "    Out:\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "          0   0.267101  0.645669  0.377880       127\n",
    "          1   0.361905  0.290076  0.322034       131\n",
    "          2   0.408451  0.243697  0.305263       119\n",
    "          3   0.345455  0.327586  0.336283       116\n",
    "          4   0.445652  0.333333  0.381395       123\n",
    "          5   0.413793  0.095238  0.154839       126\n",
    "          6   0.428571  0.474820  0.450512       139\n",
    "          7   0.446809  0.169355  0.245614       124\n",
    "          8   0.302703  0.466667  0.367213       120\n",
    "          9   0.373333  0.448000  0.407273       125\n",
    "\n",
    "    avg / total   0.379944  0.351200  0.335989      1250\n",
    "        \n",
    "    print(report_with_auc)\n",
    "    \n",
    "    Out:\n",
    "                precision    recall  f1-score  support    pred       AUC\n",
    "    0             0.267101  0.645669  0.377880    127.0   307.0  0.810550\n",
    "    1             0.361905  0.290076  0.322034    131.0   105.0  0.777579\n",
    "    2             0.408451  0.243697  0.305263    119.0    71.0  0.823277\n",
    "    3             0.345455  0.327586  0.336283    116.0   110.0  0.844390\n",
    "    4             0.445652  0.333333  0.381395    123.0    92.0  0.811389\n",
    "    5             0.413793  0.095238  0.154839    126.0    29.0  0.654790\n",
    "    6             0.428571  0.474820  0.450512    139.0   154.0  0.876458\n",
    "    7             0.446809  0.169355  0.245614    124.0    47.0  0.777237\n",
    "    8             0.302703  0.466667  0.367213    120.0   185.0  0.799735\n",
    "    9             0.373333  0.448000  0.407273    125.0   150.0  0.825959\n",
    "    avg / total   0.379944  0.351200  0.335989   1250.0  1250.0  0.800534\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    if y_true.shape != y_pred.shape:\n",
    "        print(\"Error! y_true %s is not the same shape as y_pred %s\" % (\n",
    "              y_true.shape,\n",
    "              y_pred.shape)\n",
    "        )\n",
    "        return\n",
    "    \n",
    "    lb = LabelBinarizer()\n",
    "    \n",
    "    if len(y_true.shape) == 1:\n",
    "        lb.fit(y_true)\n",
    "    \n",
    "    #Value counts of predictions\n",
    "    labels, cnt = np.unique(\n",
    "        y_pred,\n",
    "        return_counts=True)\n",
    "    n_classes = len(labels)\n",
    "    all_labels = set(labels).union(np.unique(y_true))\n",
    "    \n",
    "    pred_cnt = pd.Series(cnt, index=labels)\n",
    "    pred_cnt = pred_cnt.loc[\n",
    "        all_labels\n",
    "    ].fillna(0)\n",
    "    \n",
    "    metrics_summary = precision_recall_fscore_support(\n",
    "            y_true=y_true,\n",
    "            y_pred=y_pred,\n",
    "            labels=list(all_labels))\n",
    "\n",
    "    avg = list(precision_recall_fscore_support(\n",
    "            y_true=y_true, \n",
    "            y_pred=y_pred,\n",
    "            average='weighted'))\n",
    "\n",
    "    metrics_sum_index = ['precision', 'recall', 'f1-score', 'support']\n",
    "    class_report_df = pd.DataFrame(\n",
    "        list(metrics_summary),\n",
    "        index=metrics_sum_index,\n",
    "        columns=all_labels)\n",
    "    \n",
    "    for l in (all_labels - set(class_report_df.columns)):\n",
    "        class_report_df[l] = 0\n",
    "    \n",
    "    support = class_report_df.loc['support']\n",
    "    total = support.sum() \n",
    "    class_report_df['avg / total'] = avg[:-1] + [total]\n",
    "    \n",
    "    class_report_df = class_report_df.T\n",
    "    class_report_df['pred'] = pred_cnt\n",
    "    class_report_df['pred'].iloc[-1] = total\n",
    "    \n",
    "    if not (y_score is None):\n",
    "        fpr = dict()\n",
    "        tpr = dict()\n",
    "        roc_auc = dict()\n",
    "        for label_it, label in enumerate(labels):\n",
    "            fpr[label], tpr[label], _ = roc_curve(\n",
    "                (y_true == label).astype(int), \n",
    "                y_score[:, label_it])\n",
    "            \n",
    "            roc_auc[label] = auc(fpr[label], tpr[label])\n",
    "        \n",
    "        if average == 'micro':\n",
    "            if n_classes <= 2:\n",
    "                fpr[\"avg / total\"], tpr[\"avg / total\"], _ = roc_curve(\n",
    "                    lb.transform(y_true).ravel(), \n",
    "                    y_score[:, 1].ravel())\n",
    "            else:\n",
    "                fpr[\"avg / total\"], tpr[\"avg / total\"], _ = roc_curve(\n",
    "                        lb.transform(y_true).ravel(), \n",
    "                        y_score.ravel())\n",
    "            \n",
    "            roc_auc[\"avg / total\"] = auc(\n",
    "                fpr[\"avg / total\"], \n",
    "                tpr[\"avg / total\"])\n",
    "        \n",
    "        elif average == 'macro':\n",
    "            # First aggregate all false positive rates\n",
    "            all_fpr = np.unique(np.concatenate([\n",
    "                fpr[i] for i in labels]\n",
    "            ))\n",
    "            \n",
    "            # Then interpolate all ROC curves at this points\n",
    "            mean_tpr = np.zeros_like(all_fpr)\n",
    "            for i in labels:\n",
    "                mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "            \n",
    "            # Finally average it and compute AUC\n",
    "            mean_tpr /= n_classes\n",
    "            \n",
    "            fpr[\"macro\"] = all_fpr\n",
    "            tpr[\"macro\"] = mean_tpr\n",
    "            \n",
    "            roc_auc[\"avg / total\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "        \n",
    "        class_report_df['AUC'] = pd.Series(roc_auc)\n",
    "    \n",
    "    return class_report_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python2\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Aug  9 14:25:31 2018\n",
    "\n",
    "@author: lsanchez\n",
    "\"\"\"\n",
    "\n",
    "# from EENN import EntEmbNN\n",
    "# import eval_utils\n",
    "\n",
    "\n",
    "class EntEmbNNRegression(EntEmbNN):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    cat_emb_dim : dict\n",
    "        Dictionary containing the embedding sizes.\n",
    "\n",
    "    layers : list\n",
    "        NN. Layer arquitecture\n",
    "    drop_out_layers : dict\n",
    "        Dictionary with layer dropout\n",
    "    drop_out_emb : float\n",
    "        embedding drop out\n",
    "    batch_size : int\n",
    "        Mini Batch size\n",
    "    val_idx : list\n",
    "\n",
    "    allow_cuda : bool\n",
    "\n",
    "    act_func : string\n",
    "\n",
    "    lr : float\n",
    "\n",
    "    alpha : float\n",
    "\n",
    "    epochs : int\n",
    "    '''\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        cat_emb_dim = {},\n",
    "        dense_layers = [1000, 500],\n",
    "        drop_out_layers = [0., 0.],\n",
    "        drop_out_emb = 0.,\n",
    "        act_func = 'relu',\n",
    "        loss_function='MSELoss',\n",
    "        train_size=1.,\n",
    "        batch_size=128,\n",
    "        epochs=10,\n",
    "        lr=0.001,\n",
    "        alpha=0.0,\n",
    "        rand_seed=1,\n",
    "        allow_cuda=False,\n",
    "        random_seed=None,\n",
    "        output_sigmoid=False,\n",
    "        verbose=False):\n",
    "\n",
    "        super(EntEmbNNRegression, self).__init__()\n",
    "\n",
    "        # Model specific params.\n",
    "        self.cat_emb_dim = cat_emb_dim\n",
    "        self.dense_layers = dense_layers\n",
    "        self.output_sigmoid = output_sigmoid\n",
    "\n",
    "        # Reg. parameters\n",
    "        self.drop_out_layers = drop_out_layers\n",
    "        self.drop_out_emb = drop_out_emb\n",
    "        self.alpha = alpha\n",
    "\n",
    "        # Training params\n",
    "        self.act_func = act_func\n",
    "        self.train_size = train_size\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.loss_function = loss_function\n",
    "\n",
    "        # Misc\n",
    "        self.allow_cuda = allow_cuda\n",
    "        self.verbose = verbose\n",
    "        self.random_seed = random_seed\n",
    "\n",
    "        # Internal\n",
    "        self.embeddings = {}\n",
    "        self.train_loss = []\n",
    "        self.epochs_reports = []\n",
    "\n",
    "        self.labelencoders = {}\n",
    "        self.scaler = None\n",
    "\n",
    "        self.num_features = None\n",
    "        self.cat_features = None\n",
    "        self.layers = {}\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "\n",
    "        return self.predict_raw(X)\n",
    "\n",
    "    def eval_model(self):\n",
    "        '''\n",
    "        Model evaluation\n",
    "        '''\n",
    "\n",
    "        self.eval()\n",
    "\n",
    "        test_y_pred = self.predict(self.X_test)\n",
    "\n",
    "        report = eval_regression(\n",
    "            y_true=self.y_test,\n",
    "            y_pred=test_y_pred)\n",
    "\n",
    "        msg = \"\\t[%s] Test: MSE:%s MAE: %s gini: %s R2: %s MAPE: %s\"\n",
    "\n",
    "        msg_params = (\n",
    "            self.epoch_cnt,\n",
    "            round(report['mean_squared_error'], 6),\n",
    "            round(report['mean_absolute_error'], 6),\n",
    "            round(report['gini_normalized'], 6),\n",
    "            round(report['r2_score'], 6),\n",
    "            round(report['mean_absolute_percentage_error'], 6))\n",
    "\n",
    "        self.epochs_reports.append(report)\n",
    "\n",
    "        if self.verbose:\n",
    "            print(msg % (msg_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# import datasets\n",
    "# import eval_utils\n",
    "import numpy as np\n",
    "\n",
    "# from EENNRegression import EntEmbNNRegression\n",
    "\n",
    "# X, y, X_test, y_test = datasets.get_X_train_test_data()\n",
    "X, X_test,y, y_test = train_test_split(train_data_x, train_data_y, test_size=0.1)\n",
    "\n",
    "# This normalization comes from original Entity Emb. original Code\n",
    "# y = pd.DataFrame(y)\n",
    "# y_test = pd.DataFrame(y_test)\n",
    "y_max = max(y.max(axis=None), y_test.max(axis=None))\n",
    "y = np.log(y) / np.log(y_max)\n",
    "y_test = np.log(y_test) / np.log(y_max)\n",
    "# X = pd.DataFrame(X)\n",
    "# X_test = pd.DataFrame(X_test)\n",
    "# print(X.shape)\n",
    "# X_test.head(10)\n",
    "# y_test.head(10)\n",
    "for data in [X, X_test]:\n",
    "    data.drop('Open', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "estimator requires y to be passed, but the target y is None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/aisha/Desktop/ML Proj/Proj_ML.ipynb Cell 14\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aisha/Desktop/ML%20Proj/Proj_ML.ipynb#X30sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     models\u001b[39m.\u001b[39mappend(m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aisha/Desktop/ML%20Proj/Proj_ML.ipynb#X30sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/aisha/Desktop/ML%20Proj/Proj_ML.ipynb#X30sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m test_y_pred \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([model\u001b[39m.\u001b[39;49mpredict(X_test) \u001b[39mfor\u001b[39;49;00m model \u001b[39min\u001b[39;49;00m models])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aisha/Desktop/ML%20Proj/Proj_ML.ipynb#X30sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m test_y_pred \u001b[39m=\u001b[39m test_y_pred\u001b[39m.\u001b[39mmean(axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aisha/Desktop/ML%20Proj/Proj_ML.ipynb#X30sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mMAPE: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m MAPE(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aisha/Desktop/ML%20Proj/Proj_ML.ipynb#X30sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     y_true\u001b[39m=\u001b[39my_test\u001b[39m.\u001b[39mvalues\u001b[39m.\u001b[39mflatten(),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aisha/Desktop/ML%20Proj/Proj_ML.ipynb#X30sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     y_pred\u001b[39m=\u001b[39mtest_y_pred))\n",
      "\u001b[1;32m/Users/aisha/Desktop/ML Proj/Proj_ML.ipynb Cell 14\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aisha/Desktop/ML%20Proj/Proj_ML.ipynb#X30sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     models\u001b[39m.\u001b[39mappend(m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aisha/Desktop/ML%20Proj/Proj_ML.ipynb#X30sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/aisha/Desktop/ML%20Proj/Proj_ML.ipynb#X30sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m test_y_pred \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([model\u001b[39m.\u001b[39;49mpredict(X_test) \u001b[39mfor\u001b[39;00m model \u001b[39min\u001b[39;00m models])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aisha/Desktop/ML%20Proj/Proj_ML.ipynb#X30sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m test_y_pred \u001b[39m=\u001b[39m test_y_pred\u001b[39m.\u001b[39mmean(axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aisha/Desktop/ML%20Proj/Proj_ML.ipynb#X30sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mMAPE: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m MAPE(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aisha/Desktop/ML%20Proj/Proj_ML.ipynb#X30sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     y_true\u001b[39m=\u001b[39my_test\u001b[39m.\u001b[39mvalues\u001b[39m.\u001b[39mflatten(),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aisha/Desktop/ML%20Proj/Proj_ML.ipynb#X30sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     y_pred\u001b[39m=\u001b[39mtest_y_pred))\n",
      "\u001b[1;32m/Users/aisha/Desktop/ML Proj/Proj_ML.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aisha/Desktop/ML%20Proj/Proj_ML.ipynb#X30sZmlsZQ%3D%3D?line=96'>97</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aisha/Desktop/ML%20Proj/Proj_ML.ipynb#X30sZmlsZQ%3D%3D?line=97'>98</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aisha/Desktop/ML%20Proj/Proj_ML.ipynb#X30sZmlsZQ%3D%3D?line=98'>99</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/aisha/Desktop/ML%20Proj/Proj_ML.ipynb#X30sZmlsZQ%3D%3D?line=100'>101</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict_raw(X)\n",
      "\u001b[1;32m/Users/aisha/Desktop/ML Proj/Proj_ML.ipynb Cell 14\u001b[0m line \u001b[0;36m4\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/aisha/Desktop/ML%20Proj/Proj_ML.ipynb#X30sZmlsZQ%3D%3D?line=484'>485</a>\u001b[0m \u001b[39m#Set pytorch model in eval. mode\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/aisha/Desktop/ML%20Proj/Proj_ML.ipynb#X30sZmlsZQ%3D%3D?line=485'>486</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meval()\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/aisha/Desktop/ML%20Proj/Proj_ML.ipynb#X30sZmlsZQ%3D%3D?line=487'>488</a>\u001b[0m dataloader \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmake_dataloader(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mX_transform(X))\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/aisha/Desktop/ML%20Proj/Proj_ML.ipynb#X30sZmlsZQ%3D%3D?line=489'>490</a>\u001b[0m y_pred \u001b[39m=\u001b[39m []\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/aisha/Desktop/ML%20Proj/Proj_ML.ipynb#X30sZmlsZQ%3D%3D?line=490'>491</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch_idx, (x, _) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloader):\n",
      "\u001b[1;32m/Users/aisha/Desktop/ML Proj/Proj_ML.ipynb Cell 14\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aisha/Desktop/ML%20Proj/Proj_ML.ipynb#X30sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(y, pd\u001b[39m.\u001b[39mDataFrame):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aisha/Desktop/ML%20Proj/Proj_ML.ipynb#X30sZmlsZQ%3D%3D?line=88'>89</a>\u001b[0m     y \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mvalues\u001b[39m.\u001b[39mravel()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/aisha/Desktop/ML%20Proj/Proj_ML.ipynb#X30sZmlsZQ%3D%3D?line=90'>91</a>\u001b[0m check_X_y(X, y)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aisha/Desktop/ML%20Proj/Proj_ML.ipynb#X30sZmlsZQ%3D%3D?line=92'>93</a>\u001b[0m loader \u001b[39m=\u001b[39m data_utils\u001b[39m.\u001b[39mDataLoader(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aisha/Desktop/ML%20Proj/Proj_ML.ipynb#X30sZmlsZQ%3D%3D?line=93'>94</a>\u001b[0m     data_utils\u001b[39m.\u001b[39mTensorDataset(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aisha/Desktop/ML%20Proj/Proj_ML.ipynb#X30sZmlsZQ%3D%3D?line=94'>95</a>\u001b[0m         torch\u001b[39m.\u001b[39mfrom_numpy(X\u001b[39m.\u001b[39mvalues)\u001b[39m.\u001b[39mfloat(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aisha/Desktop/ML%20Proj/Proj_ML.ipynb#X30sZmlsZQ%3D%3D?line=98'>99</a>\u001b[0m     shuffle\u001b[39m=\u001b[39mshuffle,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/aisha/Desktop/ML%20Proj/Proj_ML.ipynb#X30sZmlsZQ%3D%3D?line=99'>100</a>\u001b[0m     num_workers\u001b[39m=\u001b[39mnum_workers)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/aisha/Desktop/ML%20Proj/Proj_ML.ipynb#X30sZmlsZQ%3D%3D?line=101'>102</a>\u001b[0m \u001b[39mreturn\u001b[39;00m loader\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/utils/validation.py:1142\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1140\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1141\u001b[0m         estimator_name \u001b[39m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m-> 1142\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1143\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m requires y to be passed, but the target y is None\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1144\u001b[0m     )\n\u001b[1;32m   1146\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[1;32m   1147\u001b[0m     X,\n\u001b[1;32m   1148\u001b[0m     accept_sparse\u001b[39m=\u001b[39maccept_sparse,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1159\u001b[0m     input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1160\u001b[0m )\n\u001b[1;32m   1162\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric, estimator\u001b[39m=\u001b[39mestimator)\n",
      "\u001b[0;31mValueError\u001b[0m: estimator requires y to be passed, but the target y is None"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "for _ in range(5):\n",
    "    m = EntEmbNNRegression(\n",
    "        cat_emb_dim={\n",
    "            'Store': 10,\n",
    "            'DayOfWeek': 6,\n",
    "            'Promo': 1,\n",
    "            'Year': 2,\n",
    "            'Month': 6,\n",
    "            'Day': 10,\n",
    "            'State': 6},\n",
    "        alpha=0,\n",
    "        dense_layers=[1000, 500],\n",
    "        drop_out_layers=[0., 0.],\n",
    "        drop_out_emb=0.,\n",
    "        loss_function='L1Loss',\n",
    "        train_size=1., \n",
    "        verbose=True)\n",
    "\n",
    "    m.fit(X, y)\n",
    "    models.append(m)\n",
    "    print('\\n')\n",
    "\n",
    "test_y_pred = np.array([model.predict(X_test) for model in models])\n",
    "test_y_pred = test_y_pred.mean(axis=0)\n",
    "\n",
    "print('MAPE: %s' % MAPE(\n",
    "    y_true=y_test.values.flatten(),\n",
    "    y_pred=test_y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
