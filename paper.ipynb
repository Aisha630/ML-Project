{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Store', 'DayOfWeek', 'Date', 'Sales', 'Customers', 'Open', 'Promo', 'StateHoliday', 'SchoolHoliday']\n",
      "[{'Store': '1115', 'DayOfWeek': '2', 'Date': '2013-01-01', 'Sales': '0', 'Customers': '0', 'Open': '0', 'Promo': '0', 'StateHoliday': 'a', 'SchoolHoliday': '1'}, {'Store': '1114', 'DayOfWeek': '2', 'Date': '2013-01-01', 'Sales': '0', 'Customers': '0', 'Open': '0', 'Promo': '0', 'StateHoliday': 'a', 'SchoolHoliday': '1'}, {'Store': '1113', 'DayOfWeek': '2', 'Date': '2013-01-01', 'Sales': '0', 'Customers': '0', 'Open': '0', 'Promo': '0', 'StateHoliday': 'a', 'SchoolHoliday': '1'}]\n",
      "['Store', 'StoreType', 'Assortment', 'CompetitionDistance', 'CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear', 'Promo2', 'Promo2SinceWeek', 'Promo2SinceYear', 'PromoInterval']\n",
      "['Store', 'State']\n",
      "[{'Store': '1', 'StoreType': 'c', 'Assortment': 'a', 'CompetitionDistance': '1270', 'CompetitionOpenSinceMonth': '9', 'CompetitionOpenSinceYear': '2008', 'Promo2': '0', 'Promo2SinceWeek': '0', 'Promo2SinceYear': '0', 'PromoInterval': '0', 'State': 'HE'}, {'Store': '2', 'StoreType': 'a', 'Assortment': 'a', 'CompetitionDistance': '570', 'CompetitionOpenSinceMonth': '11', 'CompetitionOpenSinceYear': '2007', 'Promo2': '1', 'Promo2SinceWeek': '13', 'Promo2SinceYear': '2010', 'PromoInterval': 'Jan,Apr,Jul,Oct', 'State': 'TH'}]\n",
      "Shape of train_data before filtering:  1017209\n",
      "Number of train datapoints:  844338\n",
      "Shape of X is  8\n",
      "(844338, 8)\n",
      "['1' '1097' '2' '0' '2013' '1' '1' 'RP']\n",
      "[  0 109   1   0   0   0   0   7]\n",
      "[[   0  109    1    0    0    0    0    7]\n",
      " [   0 1058    1    0    0    0    0    1]\n",
      " [   0  859    1    0    0    0    0    6]\n",
      " [   0  820    1    0    0    0    0    6]\n",
      " [   0  763    1    0    0    0    0    0]\n",
      " [   0  756    1    0    0    0    0    4]\n",
      " [   0  630    1    0    0    0    0    3]\n",
      " [   0  595    1    0    0    0    0    8]\n",
      " [   0  575    1    0    0    0    0    2]\n",
      " [   0  554    1    0    0    0    0    0]] 5961\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pickle\n",
    "import csv\n",
    "\n",
    "\n",
    "def csv2dicts(csvfile):\n",
    "    data = []\n",
    "    keys = []\n",
    "    for row_index, row in enumerate(csvfile):\n",
    "        if row_index == 0:\n",
    "            keys = row\n",
    "            print(row)\n",
    "            continue\n",
    "        # if row_index % 10000 == 0:\n",
    "        #     print(row_index)\n",
    "        data.append({key: value for key, value in zip(keys, row)})\n",
    "    return data\n",
    "\n",
    "\n",
    "def set_nan_as_string(data, replace_str='0'):\n",
    "    for i, x in enumerate(data):\n",
    "        for key, value in x.items():\n",
    "            if value == '':\n",
    "                x[key] = replace_str\n",
    "        data[i] = x\n",
    "\n",
    "\n",
    "train_data = \"rossmann-store-sales/train.csv\"\n",
    "store_data = \"rossmann-store-sales/store.csv\"\n",
    "store_states = 'rossmann-store-sales/store_states.csv'\n",
    "\n",
    "with open(train_data) as csvfile:\n",
    "    data = csv.reader(csvfile, delimiter=',')\n",
    "    with open('train_data.pickle', 'wb') as f:\n",
    "        data = csv2dicts(data)\n",
    "        data = data[::-1]\n",
    "        pickle.dump(data, f, -1)\n",
    "        print(data[:3])\n",
    "\n",
    "\n",
    "with open(store_data) as csvfile, open(store_states) as csvfile2:\n",
    "    data = csv.reader(csvfile, delimiter=',')\n",
    "    state_data = csv.reader(csvfile2, delimiter=',')\n",
    "    with open('store_data.pickle', 'wb') as f:\n",
    "        data = csv2dicts(data)\n",
    "        state_data = csv2dicts(state_data)\n",
    "        set_nan_as_string(data)\n",
    "        for index, val in enumerate(data):\n",
    "            state = state_data[index]\n",
    "            val['State'] = state['State']\n",
    "            data[index] = val\n",
    "        pickle.dump(data, f, -1)\n",
    "        print(data[:2])\n",
    "        \n",
    "        \n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "with open('train_data.pickle', 'rb') as f:\n",
    "    train_data = pickle.load(f)\n",
    "    num_records = len(train_data)\n",
    "with open('store_data.pickle', 'rb') as f:\n",
    "    store_data = pickle.load(f)\n",
    "\n",
    "\n",
    "def feature_list(record):\n",
    "    dt = datetime.strptime(record['Date'], '%Y-%m-%d')\n",
    "    store_index = int(record['Store'])\n",
    "    year = dt.year\n",
    "    month = dt.month\n",
    "    day = dt.day\n",
    "    day_of_week = int(record['DayOfWeek'])\n",
    "    try:\n",
    "        store_open = int(record['Open'])\n",
    "    except:\n",
    "        store_open = 1\n",
    "\n",
    "    promo = int(record['Promo'])\n",
    "\n",
    "    return [store_open,  # they only kept these columns\n",
    "            store_index,\n",
    "            day_of_week,\n",
    "            promo,\n",
    "            year,\n",
    "            month,\n",
    "            day,\n",
    "            store_data[store_index - 1]['State']\n",
    "            ]\n",
    "\n",
    "\n",
    "train_data_X = []\n",
    "train_data_y = []\n",
    "print(\"Shape of train_data before filtering: \", len(train_data))\n",
    "for record in train_data:\n",
    "    if record['Sales'] != '0' and record['Open'] != '':\n",
    "        fl = feature_list(record)\n",
    "        train_data_X.append(fl)\n",
    "        train_data_y.append(int(record['Sales']))\n",
    "print(\"Number of train datapoints: \", len(train_data_y))\n",
    "print(\"Shape of X is \", len(train_data_X[0]))\n",
    "\n",
    "# print(min(train_data_y), max(train_data_y))\n",
    "full_X = train_data_X\n",
    "full_X = np.array(full_X)\n",
    "train_data_X = np.array(train_data_X)\n",
    "print(train_data_X.shape)\n",
    "print(train_data_X[0])\n",
    "les = []\n",
    "for i in range(train_data_X.shape[1]):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(full_X[:, i])\n",
    "    les.append(le)\n",
    "    train_data_X[:, i] = le.transform(train_data_X[:, i])\n",
    "\n",
    "with open('les.pickle', 'wb') as f:\n",
    "    pickle.dump(les, f, -1)\n",
    "\n",
    "train_data_X = train_data_X.astype(int)\n",
    "train_data_y = np.array(train_data_y)\n",
    "print(train_data_X[0])\n",
    "print(train_data_X[0:10], train_data_y[0])\n",
    "with open('feature_train_data.pickle', 'wb') as f:\n",
    "    pickle.dump((train_data_X, train_data_y), f, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "numpy.random.seed(123)\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "from sklearn import neighbors\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model as KerasModel\n",
    "from keras.layers import Input, Dense, Activation, Reshape\n",
    "from keras.layers import Concatenate\n",
    "from keras.layers import Embedding\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "def embed_features(X, saved_embeddings_fname):\n",
    "    # f_embeddings = open(\"embeddings_shuffled.pickle\", \"rb\")\n",
    "    f_embeddings = open(saved_embeddings_fname, \"rb\")\n",
    "    embeddings = pickle.load(f_embeddings)\n",
    "\n",
    "    index_embedding_mapping = {1: 0, 2: 1, 4: 2, 5: 3, 6: 4, 7: 5}\n",
    "    X_embedded = []\n",
    "\n",
    "    (num_records, num_features) = X.shape\n",
    "    print(\"Shape of X in embed features is \", X.shape)\n",
    "    for record in X:\n",
    "        embedded_features = []\n",
    "        for i, feat in enumerate(record):\n",
    "            feat = int(feat)\n",
    "            if i not in index_embedding_mapping.keys():\n",
    "                embedded_features += [feat]\n",
    "            else:\n",
    "                embedding_index = index_embedding_mapping[i]\n",
    "                embedded_features += embeddings[embedding_index][feat].tolist()\n",
    "\n",
    "        X_embedded.append(embedded_features)\n",
    "        \n",
    "    \n",
    "\n",
    "    return numpy.array(X_embedded)\n",
    "\n",
    "\n",
    "def split_features(X): # they implicitly drop the first column Open\n",
    "    X_list = []\n",
    "\n",
    "    store_index = X[..., [1]]\n",
    "    X_list.append(store_index)\n",
    "\n",
    "    day_of_week = X[..., [2]]\n",
    "    X_list.append(day_of_week)\n",
    "\n",
    "    promo = X[..., [3]]\n",
    "    X_list.append(promo)\n",
    "\n",
    "    year = X[..., [4]]\n",
    "    X_list.append(year)\n",
    "\n",
    "    month = X[..., [5]]\n",
    "    X_list.append(month)\n",
    "\n",
    "    day = X[..., [6]]\n",
    "    X_list.append(day)\n",
    "\n",
    "    State = X[..., [7]]\n",
    "    X_list.append(State)\n",
    "\n",
    "    return X_list\n",
    "\n",
    "\n",
    "class NN_with_EntityEmbedding(Model):\n",
    "\n",
    "    def __init__(self, X_train, y_train, X_val, y_val):\n",
    "        super().__init__()\n",
    "        self.epochs = 10\n",
    "        self.checkpointer = ModelCheckpoint(filepath=\"best_model_weights.hdf5\", verbose=1, save_best_only=True)\n",
    "        self.max_log_y = max(numpy.max(numpy.log(y_train)), numpy.max(numpy.log(y_val)))\n",
    "        self.__build_keras_model()\n",
    "        self.fit(X_train, y_train, X_val, y_val)\n",
    "\n",
    "    def preprocessing(self, X):\n",
    "        X_list = split_features(X)\n",
    "        # print(len(X_list))\n",
    "        # print(X_list[0].shape)\n",
    "        \n",
    "        # print(X_list[:3])\n",
    "        return X_list # returns a list with each row being numoy array\n",
    "\n",
    "    def __build_keras_model(self):\n",
    "        input_store = Input(shape=(1,))\n",
    "        output_store = Embedding(1115, 10, name='store_embedding')(input_store)\n",
    "        output_store = Reshape(target_shape=(10,))(output_store)\n",
    "        print(\"Shape of output store is \", output_store.shape)\n",
    "\n",
    "        input_dow = Input(shape=(1,))\n",
    "        output_dow = Embedding(7, 6, name='dow_embedding')(input_dow)\n",
    "        output_dow = Reshape(target_shape=(6,))(output_dow)\n",
    "        print(\"Shape of output dow is \", output_dow.shape)\n",
    "\n",
    "        input_promo = Input(shape=(1,))\n",
    "        output_promo = Dense(1)(input_promo)\n",
    "        print(\"Shape of output promo is \", output_promo.shape)\n",
    "\n",
    "        input_year = Input(shape=(1,))\n",
    "        output_year = Embedding(3, 2, name='year_embedding')(input_year)\n",
    "        output_year = Reshape(target_shape=(2,))(output_year)\n",
    "\n",
    "        input_month = Input(shape=(1,))\n",
    "        output_month = Embedding(12, 6, name='month_embedding')(input_month)\n",
    "        output_month = Reshape(target_shape=(6,))(output_month)\n",
    "\n",
    "        input_day = Input(shape=(1,))\n",
    "        output_day = Embedding(31, 10, name='day_embedding')(input_day)\n",
    "        output_day = Reshape(target_shape=(10,))(output_day)\n",
    "\n",
    "        input_germanstate = Input(shape=(1,))\n",
    "        output_germanstate = Embedding(12, 6, name='state_embedding')(input_germanstate)\n",
    "        output_germanstate = Reshape(target_shape=(6,))(output_germanstate)\n",
    "\n",
    "        input_model = [input_store, input_dow, input_promo,\n",
    "                       input_year, input_month, input_day, input_germanstate]\n",
    "\n",
    "        output_embeddings = [output_store, output_dow, output_promo,\n",
    "                             output_year, output_month, output_day, output_germanstate]\n",
    "\n",
    "        output_model = Concatenate()(output_embeddings)\n",
    "        output_model = Dense(1000, kernel_initializer=\"uniform\")(output_model)\n",
    "        output_model = Activation('relu')(output_model)\n",
    "        output_model = Dense(500, kernel_initializer=\"uniform\")(output_model)\n",
    "        output_model = Activation('relu')(output_model)\n",
    "        output_model = Dense(1)(output_model)\n",
    "        output_model = Activation('sigmoid')(output_model)\n",
    "\n",
    "        self.model = KerasModel(inputs=input_model, outputs=output_model)\n",
    "\n",
    "        self.model.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "\n",
    "    def _val_for_fit(self, val):\n",
    "        val = numpy.log(val) / self.max_log_y\n",
    "        return val\n",
    "\n",
    "    def _val_for_pred(self, val):\n",
    "        return numpy.exp(val * self.max_log_y)\n",
    "\n",
    "    def fit(self, X_train, y_train, X_val, y_val):\n",
    "        print(\"Fit shape \", X_train.shape)  \n",
    "        self.model.fit(self.preprocessing(X_train), self._val_for_fit(y_train),\n",
    "                       validation_data=(self.preprocessing(X_val), self._val_for_fit(y_val)),\n",
    "                       epochs=self.epochs, batch_size=128,\n",
    "                       # callbacks=[self.checkpointer],\n",
    "                       )\n",
    "        # self.model.load_weights('best_model_weights.hdf5')\n",
    "        print(\"Result on validation data: \", self.evaluate(X_val, y_val))\n",
    "\n",
    "    def guess(self, features):\n",
    "        print(\"Shape of features is \", features.shape)\n",
    "        features = self.preprocessing(features)\n",
    "        result = self.model.predict(features).flatten()\n",
    "        return self._val_for_pred(result)\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data (844338, 8) (844338,)\n",
      "Number of samples used for training: 20000\n",
      "(20000, 8)\n",
      "(20000,)\n",
      "Fitting NN_with_EntityEmbedding...\n",
      "Shape of output store is  (None, 10)\n",
      "Shape of output dow is  (None, 6)\n",
      "Shape of output promo is  (None, 1)\n",
      "Fit shape  (20000, 8)\n",
      "Epoch 1/10\n",
      "157/157 [==============================] - 3s 15ms/step - loss: 0.0373 - val_loss: 0.0149\n",
      "Epoch 2/10\n",
      "157/157 [==============================] - 2s 13ms/step - loss: 0.0143 - val_loss: 0.0140\n",
      "Epoch 3/10\n",
      "157/157 [==============================] - 2s 14ms/step - loss: 0.0134 - val_loss: 0.0134\n",
      "Epoch 4/10\n",
      "157/157 [==============================] - 2s 14ms/step - loss: 0.0123 - val_loss: 0.0125\n",
      "Epoch 5/10\n",
      "157/157 [==============================] - 2s 14ms/step - loss: 0.0111 - val_loss: 0.0128\n",
      "Epoch 6/10\n",
      "157/157 [==============================] - 2s 15ms/step - loss: 0.0106 - val_loss: 0.0126\n",
      "Epoch 7/10\n",
      "157/157 [==============================] - 2s 14ms/step - loss: 0.0106 - val_loss: 0.0125\n",
      "Epoch 8/10\n",
      "157/157 [==============================] - 2s 14ms/step - loss: 0.0098 - val_loss: 0.0123\n",
      "Epoch 9/10\n",
      "157/157 [==============================] - 2s 14ms/step - loss: 0.0096 - val_loss: 0.0121\n",
      "Epoch 10/10\n",
      "157/157 [==============================] - 2s 14ms/step - loss: 0.0094 - val_loss: 0.0119\n",
      "Shape of features is  (84434, 8)\n",
      "2639/2639 [==============================] - 4s 2ms/step\n",
      "Result on validation data:  0.1280644034599788\n",
      "Evaluate combined models...\n",
      "Training error...\n",
      "Shape of features is  (20000, 8)\n",
      "625/625 [==============================] - 1s 1ms/step\n",
      "0.09079983087755929\n",
      "Validation error...\n",
      "Shape of features is  (84434, 8)\n",
      "2639/2639 [==============================] - 4s 1ms/step\n",
      "0.1280644034599788\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy\n",
    "numpy.random.seed(123)\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import sys\n",
    "sys.setrecursionlimit(10000)\n",
    "\n",
    "train_ratio = 0.9\n",
    "shuffle_data = False\n",
    "one_hot_as_input = False\n",
    "embeddings_as_input = False\n",
    "save_embeddings = True\n",
    "saved_embeddings_fname = \"embeddings.pickle\"  # set save_embeddings to True to create this file\n",
    "\n",
    "f = open('feature_train_data.pickle', 'rb')\n",
    "(X, y) = pickle.load(f)\n",
    "\n",
    "print(\"Loaded data\", X.shape, y.shape)\n",
    "\n",
    "num_records = len(X)\n",
    "train_size = int(train_ratio * num_records)\n",
    "\n",
    "if shuffle_data:\n",
    "    print(\"Using shuffled data\")\n",
    "    sh = numpy.arange(X.shape[0])\n",
    "    numpy.random.shuffle(sh)\n",
    "    X = X[sh]\n",
    "    y = y[sh]\n",
    "\n",
    "if embeddings_as_input:\n",
    "    print(\"Using learned embeddings as input\")\n",
    "    X = embed_features(X, saved_embeddings_fname)\n",
    "    print(\"Shape of embedded \", X.shape)\n",
    "    # print(X[0])\n",
    "\n",
    "\n",
    "if one_hot_as_input:\n",
    "    print(\"Using one-hot encoding as input\")\n",
    "    enc = OneHotEncoder(sparse=False)\n",
    "    enc.fit(X)\n",
    "    X = enc.transform(X)\n",
    "\n",
    "X_train = X[:train_size]\n",
    "X_val = X[train_size:]\n",
    "y_train = y[:train_size]\n",
    "y_val = y[train_size:]\n",
    "\n",
    "\n",
    "def sample(X, y, n):\n",
    "    '''random samples'''\n",
    "    num_row = X.shape[0]\n",
    "    indices = numpy.random.randint(num_row, size=n)\n",
    "    return X[indices, :], y[indices]\n",
    "\n",
    "\n",
    "X_train, y_train = sample(X_train, y_train, 20000)  # Simulate data sparsity\n",
    "# X_train, y_train = sample(X_train, y_train, 200000)  # Simulate data sparsity\n",
    "print(\"Number of samples used for training: \" + str(y_train.shape[0]))\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "# print(X_train[:5])\n",
    "models = []\n",
    "\n",
    "print(\"Fitting NN_with_EntityEmbedding...\")\n",
    "# for i in range(5):\n",
    "models.append(NN_with_EntityEmbedding(X_train, y_train, X_val, y_val))\n",
    "\n",
    "if save_embeddings:\n",
    "    model = models[0].model\n",
    "    store_embedding = model.get_layer('store_embedding').get_weights()[0]\n",
    "    dow_embedding = model.get_layer('dow_embedding').get_weights()[0]\n",
    "    year_embedding = model.get_layer('year_embedding').get_weights()[0]\n",
    "    month_embedding = model.get_layer('month_embedding').get_weights()[0]\n",
    "    day_embedding = model.get_layer('day_embedding').get_weights()[0]\n",
    "    german_states_embedding = model.get_layer('state_embedding').get_weights()[0]\n",
    "    with open(saved_embeddings_fname, 'wb') as f:\n",
    "        pickle.dump([store_embedding, dow_embedding, year_embedding,\n",
    "                     month_embedding, day_embedding, german_states_embedding], f, -1)\n",
    "\n",
    "\n",
    "def evaluate_models(models, X, y):\n",
    "    assert(min(y) > 0)\n",
    "    guessed_sales = numpy.array([model.guess(X) for model in models])\n",
    "    mean_sales = guessed_sales.mean(axis=0)\n",
    "    relative_err = numpy.absolute((y - mean_sales) / y)\n",
    "    result = numpy.sum(relative_err) / len(y)\n",
    "    return result\n",
    "\n",
    "\n",
    "print(\"Evaluate combined models...\")\n",
    "print(\"Training error...\")\n",
    "r_train = evaluate_models(models, X_train, y_train)\n",
    "print(r_train)\n",
    "\n",
    "print(\"Validation error...\")\n",
    "r_val = evaluate_models(models, X_val, y_val)\n",
    "print(r_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
